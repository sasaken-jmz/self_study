{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624fc680-a0b8-4ae8-957d-d0c419f3d4dc",
   "metadata": {},
   "source": [
    "# 勾配ベース実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d04e387-83d3-4b40-bd8e-461a413d577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import check_random_state\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from dataset import generate_synthetic_data\n",
    "from policylearners_custom import RegBasedPolicyLearner, GradientBasedPolicyLearner, POTEC\n",
    "from utils import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18b5d590-d871-4657-a025-e222054db93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>a</th>\n",
       "      <th>r</th>\n",
       "      <th>q_x_0</th>\n",
       "      <th>q_x_1</th>\n",
       "      <th>pi_0_0</th>\n",
       "      <th>pi_0_1</th>\n",
       "      <th>pscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.204708</td>\n",
       "      <td>0.478943</td>\n",
       "      <td>-0.519439</td>\n",
       "      <td>-0.555730</td>\n",
       "      <td>1.965781</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.175144</td>\n",
       "      <td>0.042578</td>\n",
       "      <td>0.431547</td>\n",
       "      <td>0.568453</td>\n",
       "      <td>0.568453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.393406</td>\n",
       "      <td>0.092908</td>\n",
       "      <td>0.281746</td>\n",
       "      <td>0.769023</td>\n",
       "      <td>1.246435</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.357402</td>\n",
       "      <td>0.026896</td>\n",
       "      <td>0.545722</td>\n",
       "      <td>0.454278</td>\n",
       "      <td>0.545722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.007189</td>\n",
       "      <td>-1.296221</td>\n",
       "      <td>0.274992</td>\n",
       "      <td>0.228913</td>\n",
       "      <td>1.352917</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086599</td>\n",
       "      <td>0.165448</td>\n",
       "      <td>0.561380</td>\n",
       "      <td>0.438620</td>\n",
       "      <td>0.561380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.886429</td>\n",
       "      <td>-2.001637</td>\n",
       "      <td>-0.371843</td>\n",
       "      <td>1.669025</td>\n",
       "      <td>-0.438570</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.777752</td>\n",
       "      <td>0.381532</td>\n",
       "      <td>0.618468</td>\n",
       "      <td>0.381532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.539741</td>\n",
       "      <td>0.476985</td>\n",
       "      <td>3.248944</td>\n",
       "      <td>-1.021228</td>\n",
       "      <td>-0.577087</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.440605</td>\n",
       "      <td>0.559395</td>\n",
       "      <td>0.559395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>-0.112073</td>\n",
       "      <td>1.215704</td>\n",
       "      <td>-0.042718</td>\n",
       "      <td>-0.725678</td>\n",
       "      <td>1.031984</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833096</td>\n",
       "      <td>0.692429</td>\n",
       "      <td>0.505587</td>\n",
       "      <td>0.494413</td>\n",
       "      <td>0.505587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.582329</td>\n",
       "      <td>-0.722583</td>\n",
       "      <td>1.730428</td>\n",
       "      <td>0.322163</td>\n",
       "      <td>-0.022597</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994537</td>\n",
       "      <td>0.995253</td>\n",
       "      <td>0.380919</td>\n",
       "      <td>0.619081</td>\n",
       "      <td>0.619081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1.994693</td>\n",
       "      <td>-0.119278</td>\n",
       "      <td>1.594432</td>\n",
       "      <td>0.609493</td>\n",
       "      <td>1.011609</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.976250</td>\n",
       "      <td>0.739832</td>\n",
       "      <td>0.469637</td>\n",
       "      <td>0.530363</td>\n",
       "      <td>0.530363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1.580550</td>\n",
       "      <td>0.954355</td>\n",
       "      <td>-2.067756</td>\n",
       "      <td>0.349648</td>\n",
       "      <td>-2.107241</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.093793</td>\n",
       "      <td>0.416674</td>\n",
       "      <td>0.434727</td>\n",
       "      <td>0.565273</td>\n",
       "      <td>0.565273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-0.862853</td>\n",
       "      <td>2.152294</td>\n",
       "      <td>-0.006706</td>\n",
       "      <td>-1.214723</td>\n",
       "      <td>0.654989</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.536803</td>\n",
       "      <td>0.463197</td>\n",
       "      <td>0.463197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x_0       x_1       x_2       x_3       x_4  a  r     q_x_0  \\\n",
       "0    -0.204708  0.478943 -0.519439 -0.555730  1.965781  1  0  0.175144   \n",
       "1     1.393406  0.092908  0.281746  0.769023  1.246435  0  0  0.357402   \n",
       "2     1.007189 -1.296221  0.274992  0.228913  1.352917  0  0  0.086599   \n",
       "3     0.886429 -2.001637 -0.371843  1.669025 -0.438570  0  0  0.000486   \n",
       "4    -0.539741  0.476985  3.248944 -1.021228 -0.577087  1  1  1.000000   \n",
       "...        ...       ...       ...       ...       ... .. ..       ...   \n",
       "1995 -0.112073  1.215704 -0.042718 -0.725678  1.031984  0  1  0.833096   \n",
       "1996  0.582329 -0.722583  1.730428  0.322163 -0.022597  1  1  0.994537   \n",
       "1997  1.994693 -0.119278  1.594432  0.609493  1.011609  1  1  0.976250   \n",
       "1998  1.580550  0.954355 -2.067756  0.349648 -2.107241  1  1  0.093793   \n",
       "1999 -0.862853  2.152294 -0.006706 -1.214723  0.654989  1  1  0.999995   \n",
       "\n",
       "         q_x_1    pi_0_0    pi_0_1    pscore  \n",
       "0     0.042578  0.431547  0.568453  0.568453  \n",
       "1     0.026896  0.545722  0.454278  0.545722  \n",
       "2     0.165448  0.561380  0.438620  0.561380  \n",
       "3     0.777752  0.381532  0.618468  0.381532  \n",
       "4     1.000000  0.440605  0.559395  0.559395  \n",
       "...        ...       ...       ...       ...  \n",
       "1995  0.692429  0.505587  0.494413  0.505587  \n",
       "1996  0.995253  0.380919  0.619081  0.619081  \n",
       "1997  0.739832  0.469637  0.530363  0.530363  \n",
       "1998  0.416674  0.434727  0.565273  0.565273  \n",
       "1999  0.999987  0.536803  0.463197  0.463197  \n",
       "\n",
       "[2000 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'logging': 0.6082038453097033, 'ips-pg': 0.684401668237789}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## シミュレーション設定\n",
    "num_runs = 100 # シミュレーションの繰り返し回数\n",
    "dim_x = 5 # 特徴量xの次元\n",
    "num_actions = 2 # 行動数, |A|\n",
    "num_clusters = 2 # 行動クラスタ数, |C|\n",
    "lambda_ = 0.5 # クラスタ効果と残差効果の配合率\n",
    "max_iter = 31 # エポック数\n",
    "test_data_size = 50000 # テストデータのサイズ\n",
    "random_state = 12345\n",
    "torch.manual_seed(random_state)\n",
    "random_ = check_random_state(random_state)\n",
    "num_data_list = [100, 200, 500, 1000, 2000] # トレーニングデータのサイズ\n",
    "\n",
    "num_data = num_data_list[4] # 2000\n",
    "\n",
    "## 期待報酬関数を定義するためのパラメータを抽出\n",
    "phi_a = random_.choice(num_clusters, size=num_actions)\n",
    "theta_g = random_.normal(size=(dim_x, num_clusters))\n",
    "M_g = random_.normal(size=(dim_x, num_clusters))\n",
    "b_g = random_.normal(size=(1, num_clusters))\n",
    "theta_h = random_.normal(size=(dim_x, num_actions))\n",
    "M_h = random_.normal(size=(dim_x, num_actions))\n",
    "b_h = random_.normal(size=(1, num_actions))\n",
    "\n",
    "## 学習された方策の真の性能を近似するためのテストデータを生成\n",
    "test_data = generate_synthetic_data(\n",
    "    num_data=test_data_size, lambda_=lambda_,\n",
    "    theta_g=theta_g, M_g=M_g, b_g=b_g, theta_h=theta_h, M_h=M_h, b_h=b_h, phi_a=phi_a,\n",
    "    dim_context=dim_x, num_actions=num_actions, num_clusters=num_clusters, random_state = random_state\n",
    ")\n",
    "pi_0_value = (test_data[\"q_x_a\"] * test_data[\"pi_0\"]).sum(1).mean()\n",
    "\n",
    "## データ収集方策が形成する分布に従いログデータを生成\n",
    "offline_logged_data = generate_synthetic_data(\n",
    "    num_data=num_data, lambda_=lambda_,\n",
    "    theta_g=theta_g, M_g=M_g, b_g=b_g, theta_h=theta_h, M_h=M_h, b_h=b_h, phi_a=phi_a,\n",
    "    dim_context=dim_x, num_actions=num_actions, num_clusters=num_clusters,\n",
    "    # random_state = _\n",
    "    random_state = random_state\n",
    ")\n",
    "\n",
    "# 辞書型で直観的にわかりづらいのでdfに変換\n",
    "# log_data->log_data_df\n",
    "data = offline_logged_data.copy()\n",
    "df = pd.DataFrame(data['x'], columns=['x_'+str(i) for i in range(data['x'].shape[1])])\n",
    "df['a'] = data['a']\n",
    "df['r'] = data['r']\n",
    "ex_reward_df = pd.DataFrame(data['q_x_a'], columns=['q_x_'+str(i) for i in range(data['num_actions'])])\n",
    "df = pd.concat([df, ex_reward_df], axis=1)\n",
    "pi_b_df = pd.DataFrame(data['pi_0'].reshape(data['num_data'], data['num_actions']), columns=['pi_0_'+str(i) for i in range(data['num_actions'])])\n",
    "df = pd.concat([df, pi_b_df], axis=1)\n",
    "df['pscore'] = data['pscore']\n",
    "display(df)\n",
    "\n",
    "x = np.array(df[['x_0', 'x_1', 'x_2', 'x_3', 'x_4']])\n",
    "a = np.array(df['a'])\n",
    "r = np.array(df['r'])\n",
    "pscore = np.array(df['pscore'])\n",
    "pi_0 = np.array(df[['pi_0_0', 'pi_0_1']])\n",
    "\n",
    "true_value_of_learned_policies = dict()\n",
    "true_value_of_learned_policies[\"logging\"] = pi_0_value\n",
    "\n",
    "### 勾配ベースのアプローチ (IPS推定量で方策勾配を推定)\n",
    "ips = GradientBasedPolicyLearner(dim_x=dim_x, num_actions=num_actions, max_iter=max_iter)\n",
    "# ips.fit(offline_logged_data, test_data)\n",
    "ips.fit(\n",
    "    x=x,\n",
    "    a=a,\n",
    "    r=r,\n",
    "    pscore=pscore,\n",
    "    pi_0=pi_0,\n",
    "    # q_hat=q_hat\n",
    ")\n",
    "pi_ips = ips.predict(test_data)\n",
    "true_value_of_learned_policies[\"ips-pg\"] = (test_data[\"q_x_a\"] * pi_ips).sum(1).mean()\n",
    "\n",
    "test_policy_value_list = []\n",
    "test_policy_value_list.append(true_value_of_learned_policies)\n",
    "\n",
    "test_policy_value_list\n",
    "# [{'logging': 0.6082038453097033, 'ips-pg': 0.6835386885161472}] # 2,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7762b262-57f0-4ad7-b161-dea5802411ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': '0'}>]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(pi_ips[:, 1]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190512ac-e8fb-49b4-8f19-fe594ab69fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b73ecdcc-d20a-4308-9ef6-1e19f678bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policylearners_custom\n",
    "from copy import copy\n",
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import check_random_state\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from utils import softmax, RegBasedPolicyDataset, GradientBasedPolicyDataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RegBasedPolicyLearner:\n",
    "    \"\"\"回帰ベースのアプローチに基づくオフ方策学習\"\"\"\n",
    "    dim_x: int\n",
    "    num_actions: int\n",
    "    hidden_layer_size: tuple = (30, 30, 30)\n",
    "    activation: str = \"elu\"\n",
    "    batch_size: int = 16\n",
    "    learning_rate_init: float = 0.005\n",
    "    gamma: float = 0.98\n",
    "    alpha: float = 1e-6\n",
    "    log_eps: float = 1e-10\n",
    "    solver: str = \"adagrad\"\n",
    "    max_iter: int = 30\n",
    "    random_state: int = 12345\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Initialize class.\"\"\"\n",
    "        layer_list = []\n",
    "        input_size = self.dim_x\n",
    "\n",
    "        if self.activation == \"tanh\":\n",
    "            activation_layer = nn.Tanh\n",
    "        elif self.activation == \"relu\":\n",
    "            activation_layer = nn.ReLU\n",
    "        elif self.activation == \"elu\":\n",
    "            activation_layer = nn.ELU\n",
    "\n",
    "        for i, h in enumerate(self.hidden_layer_size):\n",
    "            layer_list.append((\"l{}\".format(i), nn.Linear(input_size, h)))\n",
    "            layer_list.append((\"a{}\".format(i), activation_layer()))\n",
    "            input_size = h\n",
    "        layer_list.append((\"output\", nn.Linear(input_size, self.num_actions)))\n",
    "\n",
    "        self.nn_model = nn.Sequential(OrderedDict(layer_list))\n",
    "\n",
    "        self.random_ = check_random_state(self.random_state)\n",
    "        self.train_loss = []\n",
    "        self.train_value = []\n",
    "        self.test_value = []\n",
    "\n",
    "    def fit(self, dataset: dict, dataset_test: dict) -> None:\n",
    "        x, a, r = dataset[\"x\"], dataset[\"a\"], dataset[\"r\"]\n",
    "\n",
    "        if self.solver == \"adagrad\":\n",
    "            optimizer = optim.Adagrad(\n",
    "                self.nn_model.parameters(),\n",
    "                lr=self.learning_rate_init,\n",
    "                weight_decay=self.alpha,\n",
    "            )\n",
    "        elif self.solver == \"adam\":\n",
    "            optimizer = optim.AdamW(\n",
    "                self.nn_model.parameters(),\n",
    "                lr=self.learning_rate_init,\n",
    "                weight_decay=self.alpha,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"`solver` must be one of 'adam' or 'adagrad'\")\n",
    "\n",
    "        training_data_loader = self._create_train_data_for_opl(x, a, r)\n",
    "\n",
    "        # start policy training\n",
    "        scheduler = ExponentialLR(optimizer, gamma=self.gamma)\n",
    "        q_x_a_train, q_x_a_test = dataset[\"q_x_a\"], dataset_test[\"q_x_a\"]\n",
    "        for _ in range(self.max_iter):\n",
    "            loss_epoch = 0.0\n",
    "            self.nn_model.train()\n",
    "            for x_, a_, r_ in training_data_loader:\n",
    "                optimizer.zero_grad()\n",
    "                q_hat = self.nn_model(x_)\n",
    "                idx = torch.arange(a_.shape[0], dtype=torch.long)\n",
    "                loss = ((r_ - q_hat[idx, a_]) ** 2).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_epoch += loss.item()\n",
    "            pi_train = self.predict(dataset)\n",
    "            scheduler.step()\n",
    "            self.train_value.append((q_x_a_train * pi_train).sum(1).mean())\n",
    "            pi_test = self.predict(dataset_test)\n",
    "            self.test_value.append((q_x_a_test * pi_test).sum(1).mean())\n",
    "            self.train_loss.append(loss_epoch)\n",
    "\n",
    "    def _create_train_data_for_opl(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        a: np.ndarray,\n",
    "        r: np.ndarray,\n",
    "    ) -> tuple:\n",
    "        dataset = RegBasedPolicyDataset(\n",
    "            torch.from_numpy(x).float(),\n",
    "            torch.from_numpy(a).long(),\n",
    "            torch.from_numpy(r).float(),\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def predict(self, dataset_test: np.ndarray, beta: float = 10) -> np.ndarray:\n",
    "        self.nn_model.eval()\n",
    "        x = torch.from_numpy(dataset_test[\"x\"]).float()\n",
    "        q_hat = self.nn_model(x).detach().numpy()\n",
    "\n",
    "        return softmax(beta * q_hat)\n",
    "\n",
    "    def predict_q(self, dataset_test: np.ndarray) -> np.ndarray:\n",
    "        self.nn_model.eval()\n",
    "        x = torch.from_numpy(dataset_test[\"x\"]).float()\n",
    "\n",
    "        return self.nn_model(x).detach().numpy()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GradientBasedPolicyLearner:\n",
    "    \"\"\"勾配ベースのアプローチに基づくオフ方策学習\"\"\"\n",
    "    dim_x: int\n",
    "    num_actions: int\n",
    "    hidden_layer_size: tuple = (30, 30, 30)\n",
    "    activation: str = \"elu\"\n",
    "    batch_size: int = 16\n",
    "    learning_rate_init: float = 0.005\n",
    "    gamma: float = 0.98\n",
    "    alpha: float = 1e-6\n",
    "    imit_reg: float = 0.0\n",
    "    log_eps: float = 1e-10\n",
    "    solver: str = \"adagrad\"\n",
    "    max_iter: int = 30\n",
    "    random_state: int = 12345\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Initialize class.\"\"\"\n",
    "        layer_list = []\n",
    "        input_size = self.dim_x\n",
    "\n",
    "        if self.activation == \"tanh\":\n",
    "            activation_layer = nn.Tanh\n",
    "        elif self.activation == \"relu\":\n",
    "            activation_layer = nn.ReLU\n",
    "        elif self.activation == \"elu\":\n",
    "            activation_layer = nn.ELU\n",
    "\n",
    "        for i, h in enumerate(self.hidden_layer_size):\n",
    "            layer_list.append((\"l{}\".format(i), nn.Linear(input_size, h)))\n",
    "            layer_list.append((\"a{}\".format(i), activation_layer()))\n",
    "            input_size = h\n",
    "        layer_list.append((\"output\", nn.Linear(input_size, self.num_actions)))\n",
    "        layer_list.append((\"softmax\", nn.Softmax(dim=1)))\n",
    "\n",
    "        self.nn_model = nn.Sequential(OrderedDict(layer_list))\n",
    "\n",
    "        self.random_ = check_random_state(self.random_state)\n",
    "        self.train_loss = []\n",
    "        self.train_value = []\n",
    "        self.test_value = []\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        # dataset: dict,\n",
    "        # dataset_test: dict,\n",
    "        x,\n",
    "        a,\n",
    "        r,\n",
    "        pscore,\n",
    "        pi_0,\n",
    "        q_hat: np.ndarray = None\n",
    "    ) -> None:\n",
    "        # x, a, r = dataset[\"x\"], dataset[\"a\"], dataset[\"r\"]\n",
    "        # pscore, pi_0 = dataset[\"pscore\"], dataset[\"pi_0\"]\n",
    "        x, a, r = x, a, r\n",
    "        pscore, pi_0 = pscore, pi_0\n",
    "        if q_hat is None:\n",
    "            q_hat = np.zeros((r.shape[0], self.num_actions))\n",
    "\n",
    "        if self.solver == \"adagrad\":\n",
    "            optimizer = optim.Adagrad(\n",
    "                self.nn_model.parameters(),\n",
    "                lr=self.learning_rate_init,\n",
    "                weight_decay=self.alpha,\n",
    "            )\n",
    "        elif self.solver == \"adam\":\n",
    "            optimizer = optim.AdamW(\n",
    "                self.nn_model.parameters(),\n",
    "                lr=self.learning_rate_init,\n",
    "                weight_decay=self.alpha,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"`solver` must be one of 'adam' or 'adagrad'\")\n",
    "\n",
    "        training_data_loader = self._create_train_data_for_opl(\n",
    "            x,\n",
    "            a,\n",
    "            r,\n",
    "            pscore,\n",
    "            q_hat,\n",
    "            pi_0,\n",
    "        )\n",
    "\n",
    "        # start policy training\n",
    "        scheduler = ExponentialLR(optimizer, gamma=self.gamma)\n",
    "        # q_x_a_train, q_x_a_test = dataset[\"q_x_a\"], dataset_test[\"q_x_a\"]\n",
    "        for _ in range(self.max_iter):\n",
    "            loss_epoch = 0.0\n",
    "            self.nn_model.train()\n",
    "            for x_, a_, r_, p, q_hat_, pi_0_ in training_data_loader:\n",
    "                optimizer.zero_grad()\n",
    "                pi = self.nn_model(x_)\n",
    "                loss = -self._estimate_policy_gradient(\n",
    "                    a=a_,\n",
    "                    r=r_,\n",
    "                    pscore=p,\n",
    "                    q_hat=q_hat_,\n",
    "                    pi_0=pi_0_,\n",
    "                    pi=pi,\n",
    "                ).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_epoch += loss.item()\n",
    "            self.train_loss.append(loss_epoch)\n",
    "            scheduler.step()\n",
    "            # pi_train = self.predict(dataset)\n",
    "            # self.train_value.append((q_x_a_train * pi_train).sum(1).mean())\n",
    "            # pi_test = self.predict(dataset_test)\n",
    "            # self.test_value.append((q_x_a_test * pi_test).sum(1).mean())\n",
    "\n",
    "    def _create_train_data_for_opl(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        a: np.ndarray,\n",
    "        r: np.ndarray,\n",
    "        pscore: np.ndarray,\n",
    "        q_hat: np.ndarray,\n",
    "        pi_0: np.ndarray,\n",
    "    ) -> tuple:\n",
    "        dataset = GradientBasedPolicyDataset(\n",
    "            torch.from_numpy(x).float(),\n",
    "            torch.from_numpy(a).long(),\n",
    "            torch.from_numpy(r).float(),\n",
    "            torch.from_numpy(pscore).float(),\n",
    "            torch.from_numpy(q_hat).float(),\n",
    "            torch.from_numpy(pi_0).float(),\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def _estimate_policy_gradient(\n",
    "        self,\n",
    "        a: torch.Tensor,\n",
    "        r: torch.Tensor,\n",
    "        pscore: torch.Tensor,\n",
    "        q_hat: torch.Tensor,\n",
    "        pi: torch.Tensor,\n",
    "        pi_0: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        current_pi = pi.detach()\n",
    "        log_prob = torch.log(pi + self.log_eps)\n",
    "        idx = torch.arange(a.shape[0], dtype=torch.long)\n",
    "\n",
    "        q_hat_factual = q_hat[idx, a]\n",
    "        iw = current_pi[idx, a] / pscore\n",
    "        estimated_policy_grad_arr = iw * (r - q_hat_factual) * log_prob[idx, a]\n",
    "        estimated_policy_grad_arr += torch.sum(q_hat * current_pi * log_prob, dim=1)\n",
    "\n",
    "        # imitation regularization\n",
    "        estimated_policy_grad_arr += self.imit_reg * log_prob[idx, a]\n",
    "\n",
    "        return estimated_policy_grad_arr\n",
    "\n",
    "    def predict(self, dataset_test: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        self.nn_model.eval()\n",
    "        x = torch.from_numpy(dataset_test[\"x\"]).float()\n",
    "        return self.nn_model(x).detach().numpy()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class POTEC:\n",
    "    \"\"\"回帰ベースと勾配ベースのアプローチを融合した2段階方策学習\"\"\"\n",
    "    dim_x: int\n",
    "    num_actions: int\n",
    "    num_clusters: int = 1\n",
    "    hidden_layer_size: tuple = (30, 30, 30)\n",
    "    activation: str = \"elu\"\n",
    "    batch_size: int = 16\n",
    "    learning_rate_init: float = 0.005\n",
    "    gamma: float = 0.98\n",
    "    alpha: float = 1e-6\n",
    "    log_eps: float = 1e-10\n",
    "    solver: str = \"adagrad\"\n",
    "    max_iter: int = 30\n",
    "    random_state: int = 12345\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Initialize class.\"\"\"\n",
    "        layer_list = []\n",
    "        input_size = self.dim_x\n",
    "\n",
    "        if self.activation == \"tanh\":\n",
    "            activation_layer = nn.Tanh\n",
    "        elif self.activation == \"relu\":\n",
    "            activation_layer = nn.ReLU\n",
    "        elif self.activation == \"elu\":\n",
    "            activation_layer = nn.ELU\n",
    "\n",
    "        for i, h in enumerate(self.hidden_layer_size):\n",
    "            layer_list.append((\"l{}\".format(i), nn.Linear(input_size, h)))\n",
    "            layer_list.append((\"a{}\".format(i), activation_layer()))\n",
    "            input_size = h\n",
    "        layer_list.append((\"output\", nn.Linear(input_size, self.num_clusters)))\n",
    "        layer_list.append((\"softmax\", nn.Softmax(dim=1)))\n",
    "\n",
    "        self.nn_model = nn.Sequential(OrderedDict(layer_list))\n",
    "\n",
    "        self.random_ = check_random_state(self.random_state)\n",
    "        self.train_loss = []\n",
    "        self.train_value = []\n",
    "        self.test_value = []\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        dataset: dict,\n",
    "        dataset_test: dict,\n",
    "        f_hat: np.ndarray = None,\n",
    "        f_hat_test: np.ndarray = None,\n",
    "    ) -> None:\n",
    "        x, a, r = dataset[\"x\"], dataset[\"a\"], dataset[\"r\"]\n",
    "        pscore_c, phi_a = dataset[\"pscore_c\"], torch.from_numpy(dataset[\"phi_a\"])\n",
    "        if f_hat is None:\n",
    "            f_hat = np.zeros(dataset[\"h_x_a\"].shape)\n",
    "        if f_hat_test is None:\n",
    "            f_hat_test = np.zeros(dataset_test[\"h_x_a\"].shape)\n",
    "\n",
    "        if self.solver == \"adagrad\":\n",
    "            optimizer = optim.Adagrad(\n",
    "                self.nn_model.parameters(),\n",
    "                lr=self.learning_rate_init,\n",
    "                weight_decay=self.alpha,\n",
    "            )\n",
    "        elif self.solver == \"adam\":\n",
    "            optimizer = optim.AdamW(\n",
    "                self.nn_model.parameters(),\n",
    "                lr=self.learning_rate_init,\n",
    "                weight_decay=self.alpha,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"`solver` must be one of 'adam' or 'adagrad'\")\n",
    "\n",
    "        training_data_loader = self._create_train_data_for_opl(\n",
    "            x,\n",
    "            a,\n",
    "            r,\n",
    "            pscore_c,\n",
    "            f_hat,\n",
    "            dataset[\"pi_0_c\"],\n",
    "        )\n",
    "\n",
    "        # start policy training\n",
    "        q_x_a_train, q_x_a_test = dataset[\"q_x_a\"], dataset_test[\"q_x_a\"]\n",
    "        for _ in range(self.max_iter):\n",
    "            loss_epoch = 0.0\n",
    "            self.nn_model.train()\n",
    "            for (x, a, r, p_c, f_hat_, _) in training_data_loader:\n",
    "                optimizer.zero_grad()\n",
    "                pi = self.nn_model(x)\n",
    "                loss = -self._estimate_policy_gradient(\n",
    "                    x=x,\n",
    "                    a=a,\n",
    "                    phi_a=phi_a,\n",
    "                    r=r,\n",
    "                    pscore_c=p_c,\n",
    "                    f_hat=f_hat_,\n",
    "                    pi=pi,\n",
    "                ).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_epoch += loss.item()\n",
    "            self.train_loss.append(loss_epoch)\n",
    "            pi_train = self.predict(dataset, f_hat)\n",
    "            self.train_value.append((q_x_a_train * pi_train).sum(1).mean())\n",
    "            pi_test = self.predict(dataset_test, f_hat_test)\n",
    "            self.test_value.append((q_x_a_test * pi_test).sum(1).mean())\n",
    "\n",
    "    def _create_train_data_for_opl(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        a: np.ndarray,\n",
    "        r: np.ndarray,\n",
    "        pscore_c: np.ndarray,\n",
    "        f_hat: np.ndarray,\n",
    "        pi_0_c: np.ndarray,\n",
    "    ) -> tuple:\n",
    "        dataset = GradientBasedPolicyDataset(\n",
    "            torch.from_numpy(x).float(),\n",
    "            torch.from_numpy(a).long(),\n",
    "            torch.from_numpy(r).float(),\n",
    "            torch.from_numpy(pscore_c).float(),\n",
    "            torch.from_numpy(f_hat).float(),\n",
    "            torch.from_numpy(pi_0_c).float(),\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def _estimate_policy_gradient(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        a: torch.Tensor,\n",
    "        r: torch.Tensor,\n",
    "        phi_a: torch.Tensor,\n",
    "        pscore_c: torch.Tensor,\n",
    "        f_hat: torch.Tensor,\n",
    "        pi: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        current_pi = pi.detach()\n",
    "        log_prob = torch.log(pi + self.log_eps)\n",
    "        idx = torch.arange(a.shape[0], dtype=torch.long)\n",
    "\n",
    "        f_hat_factual = f_hat[idx, a]\n",
    "        iw = current_pi[idx, phi_a[a]] / pscore_c\n",
    "        estimated_policy_grad_arr = iw * (r - f_hat_factual) * log_prob[idx, phi_a[a]]\n",
    "\n",
    "        f_hat_c = torch.zeros((a.shape[0], self.num_clusters))\n",
    "        for c in range(self.num_clusters):\n",
    "            if (phi_a == c).sum() > 0:\n",
    "                f_hat_c[:, c] = f_hat[:, phi_a == c].max(1)[0]\n",
    "            else:\n",
    "                f_hat_c[:, c] = 0.0\n",
    "        estimated_policy_grad_arr += torch.sum(f_hat_c * current_pi * log_prob, dim=1)\n",
    "\n",
    "        return estimated_policy_grad_arr\n",
    "\n",
    "    def predict(self, dataset_test: dict, f_hat_test: np.ndarray) -> np.ndarray:\n",
    "        self.nn_model.eval()\n",
    "        x = torch.from_numpy(dataset_test[\"x\"]).float()\n",
    "        pi_c = self.nn_model(x).detach().numpy()\n",
    "        phi_a = torch.from_numpy(dataset_test[\"phi_a\"])\n",
    "\n",
    "        n = x.shape[0]\n",
    "        action_set = np.arange(f_hat_test.shape[1])\n",
    "        overall_policy = np.zeros(f_hat_test.shape)\n",
    "        for c in range(self.num_clusters):\n",
    "            if (phi_a == c).sum() > 0:\n",
    "                best_actions_given_clusters = action_set[phi_a == c][\n",
    "                    f_hat_test[:, phi_a == c].argmax(1)\n",
    "                ]\n",
    "                overall_policy[np.arange(n), best_actions_given_clusters] = pi_c[:, c]\n",
    "\n",
    "        return overall_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a4526f-0193-46ab-8622-2856f6b54d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
