{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc53075-0eb0-4d76-b825-81b63d8730f0",
   "metadata": {},
   "source": [
    "# ch5_データ理解からやり直し.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b93243b-aa17-4353-8240-895bd8941402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import check_random_state\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from dataset import generate_synthetic_data\n",
    "# from policylearners import RegBasedPolicyLearner, GradientBasedPolicyLearner, POTEC\n",
    "from utils import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d54f9725-dfb1-4ab9-9c5a-738b7af6085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import check_random_state\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from utils import softmax, RegBasedPolicyDataset, GradientBasedPolicyDataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RegBasedPolicyLearner:\n",
    "    \"\"\"回帰ベースのアプローチに基づくオフ方策学習\"\"\"\n",
    "    dim_x: int\n",
    "    num_actions: int\n",
    "    hidden_layer_size: tuple = (30, 30, 30)\n",
    "    activation: str = \"elu\"\n",
    "    batch_size: int = 16\n",
    "    learning_rate_init: float = 0.005\n",
    "    gamma: float = 0.98\n",
    "    alpha: float = 1e-6\n",
    "    log_eps: float = 1e-10\n",
    "    solver: str = \"adagrad\"\n",
    "    max_iter: int = 30\n",
    "    random_state: int = 12345\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Initialize class.\"\"\"\n",
    "        layer_list = []\n",
    "        input_size = self.dim_x\n",
    "\n",
    "        if self.activation == \"tanh\":\n",
    "            activation_layer = nn.Tanh\n",
    "        elif self.activation == \"relu\":\n",
    "            activation_layer = nn.ReLU\n",
    "        elif self.activation == \"elu\":\n",
    "            activation_layer = nn.ELU\n",
    "\n",
    "        for i, h in enumerate(self.hidden_layer_size):\n",
    "            layer_list.append((\"l{}\".format(i), nn.Linear(input_size, h)))\n",
    "            layer_list.append((\"a{}\".format(i), activation_layer()))\n",
    "            input_size = h\n",
    "        layer_list.append((\"output\", nn.Linear(input_size, self.num_actions)))\n",
    "\n",
    "        self.nn_model = nn.Sequential(OrderedDict(layer_list))\n",
    "\n",
    "        self.random_ = check_random_state(self.random_state)\n",
    "        self.train_loss = []\n",
    "        self.train_value = []\n",
    "        self.test_value = []\n",
    "\n",
    "    def fit(self, dataset: dict, dataset_test: dict) -> None:\n",
    "        x, a, r = dataset[\"x\"], dataset[\"a\"], dataset[\"r\"]\n",
    "\n",
    "        if self.solver == \"adagrad\":\n",
    "            optimizer = optim.Adagrad(\n",
    "                self.nn_model.parameters(),\n",
    "                lr=self.learning_rate_init,\n",
    "                weight_decay=self.alpha,\n",
    "            )\n",
    "        elif self.solver == \"adam\":\n",
    "            optimizer = optim.AdamW(\n",
    "                self.nn_model.parameters(),\n",
    "                lr=self.learning_rate_init,\n",
    "                weight_decay=self.alpha,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"`solver` must be one of 'adam' or 'adagrad'\")\n",
    "\n",
    "        training_data_loader = self._create_train_data_for_opl(x, a, r)\n",
    "\n",
    "        # start policy training\n",
    "        scheduler = ExponentialLR(optimizer, gamma=self.gamma)\n",
    "        q_x_a_train, q_x_a_test = dataset[\"q_x_a\"], dataset_test[\"q_x_a\"]\n",
    "        for _ in range(self.max_iter):\n",
    "            loss_epoch = 0.0\n",
    "            self.nn_model.train()\n",
    "            for x_, a_, r_ in training_data_loader:\n",
    "                optimizer.zero_grad()\n",
    "                q_hat = self.nn_model(x_)\n",
    "                idx = torch.arange(a_.shape[0], dtype=torch.long)\n",
    "                loss = ((r_ - q_hat[idx, a_]) ** 2).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_epoch += loss.item()\n",
    "            pi_train = self.predict(dataset)\n",
    "            scheduler.step()\n",
    "            self.train_value.append((q_x_a_train * pi_train).sum(1).mean())\n",
    "            pi_test = self.predict(dataset_test)\n",
    "            self.test_value.append((q_x_a_test * pi_test).sum(1).mean())\n",
    "            self.train_loss.append(loss_epoch)\n",
    "\n",
    "    def _create_train_data_for_opl(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        a: np.ndarray,\n",
    "        r: np.ndarray,\n",
    "    ) -> tuple:\n",
    "        dataset = RegBasedPolicyDataset(\n",
    "            torch.from_numpy(x).float(),\n",
    "            torch.from_numpy(a).long(),\n",
    "            torch.from_numpy(r).float(),\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def predict(self, dataset_test: np.ndarray, beta: float = 10) -> np.ndarray:\n",
    "        self.nn_model.eval()\n",
    "        x = torch.from_numpy(dataset_test[\"x\"]).float()\n",
    "        q_hat = self.nn_model(x).detach().numpy()\n",
    "\n",
    "        return softmax(beta * q_hat)\n",
    "\n",
    "    def predict_q(self, dataset_test: np.ndarray) -> np.ndarray:\n",
    "        self.nn_model.eval()\n",
    "        x = torch.from_numpy(dataset_test[\"x\"]).float()\n",
    "\n",
    "        return self.nn_model(x).detach().numpy()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GradientBasedPolicyLearner:\n",
    "    \"\"\"勾配ベースのアプローチに基づくオフ方策学習\"\"\"\n",
    "    dim_x: int\n",
    "    num_actions: int\n",
    "    hidden_layer_size: tuple = (30, 30, 30)\n",
    "    activation: str = \"elu\"\n",
    "    batch_size: int = 16\n",
    "    learning_rate_init: float = 0.005\n",
    "    gamma: float = 0.98\n",
    "    alpha: float = 1e-6\n",
    "    imit_reg: float = 0.0\n",
    "    log_eps: float = 1e-10\n",
    "    solver: str = \"adagrad\"\n",
    "    max_iter: int = 30\n",
    "    random_state: int = 12345\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Initialize class.\"\"\"\n",
    "        layer_list = []\n",
    "        input_size = self.dim_x\n",
    "\n",
    "        if self.activation == \"tanh\":\n",
    "            activation_layer = nn.Tanh\n",
    "        elif self.activation == \"relu\":\n",
    "            activation_layer = nn.ReLU\n",
    "        elif self.activation == \"elu\":\n",
    "            activation_layer = nn.ELU\n",
    "\n",
    "        for i, h in enumerate(self.hidden_layer_size):\n",
    "            layer_list.append((\"l{}\".format(i), nn.Linear(input_size, h)))\n",
    "            layer_list.append((\"a{}\".format(i), activation_layer()))\n",
    "            input_size = h\n",
    "        layer_list.append((\"output\", nn.Linear(input_size, self.num_actions)))\n",
    "        layer_list.append((\"softmax\", nn.Softmax(dim=1)))\n",
    "\n",
    "        self.nn_model = nn.Sequential(OrderedDict(layer_list))\n",
    "\n",
    "        self.random_ = check_random_state(self.random_state)\n",
    "        self.train_loss = []\n",
    "        self.train_value = []\n",
    "        self.test_value = []\n",
    "\n",
    "    def fit(self, dataset: dict, dataset_test: dict, q_hat: np.ndarray = None) -> None:\n",
    "        x, a, r = dataset[\"x\"], dataset[\"a\"], dataset[\"r\"]\n",
    "        pscore, pi_0 = dataset[\"pscore\"], dataset[\"pi_0\"]\n",
    "        if q_hat is None:\n",
    "            q_hat = np.zeros((r.shape[0], self.num_actions))\n",
    "\n",
    "        if self.solver == \"adagrad\":\n",
    "            optimizer = optim.Adagrad(\n",
    "                self.nn_model.parameters(),\n",
    "                lr=self.learning_rate_init,\n",
    "                weight_decay=self.alpha,\n",
    "            )\n",
    "        elif self.solver == \"adam\":\n",
    "            optimizer = optim.AdamW(\n",
    "                self.nn_model.parameters(),\n",
    "                lr=self.learning_rate_init,\n",
    "                weight_decay=self.alpha,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"`solver` must be one of 'adam' or 'adagrad'\")\n",
    "\n",
    "        training_data_loader = self._create_train_data_for_opl(\n",
    "            x,\n",
    "            a,\n",
    "            r,\n",
    "            pscore,\n",
    "            q_hat,\n",
    "            pi_0,\n",
    "        )\n",
    "\n",
    "        # start policy training\n",
    "        scheduler = ExponentialLR(optimizer, gamma=self.gamma)\n",
    "        q_x_a_train, q_x_a_test = dataset[\"q_x_a\"], dataset_test[\"q_x_a\"]\n",
    "        _list = []\n",
    "        for _ in range(self.max_iter):\n",
    "            loss_epoch = 0.0\n",
    "            self.nn_model.train()\n",
    "            for x_, a_, r_, p, q_hat_, pi_0_ in training_data_loader:\n",
    "                optimizer.zero_grad()\n",
    "                pi = self.nn_model(x_)\n",
    "                loss = -self._estimate_policy_gradient(\n",
    "                    a=a_,\n",
    "                    r=r_,\n",
    "                    pscore=p,\n",
    "                    q_hat=q_hat_,\n",
    "                    pi_0=pi_0_,\n",
    "                    pi=pi,\n",
    "                ).mean()\n",
    "                iw, _estimated_policy_grad_arr = self._estimate_policy_gradient_2(\n",
    "                    a=a_,\n",
    "                    r=r_,\n",
    "                    pscore=p,\n",
    "                    q_hat=q_hat_,\n",
    "                    pi_0=pi_0_,\n",
    "                    pi=pi,\n",
    "                )\n",
    "                _list.append(loss)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_epoch += loss.item()\n",
    "            self.train_loss.append(loss_epoch)\n",
    "            scheduler.step()\n",
    "            pi_train = self.predict(dataset)\n",
    "            self.train_value.append((q_x_a_train * pi_train).sum(1).mean())\n",
    "            pi_test = self.predict(dataset_test)\n",
    "            self.test_value.append((q_x_a_test * pi_test).sum(1).mean())\n",
    "\n",
    "        return _list\n",
    "\n",
    "    def _create_train_data_for_opl(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        a: np.ndarray,\n",
    "        r: np.ndarray,\n",
    "        pscore: np.ndarray,\n",
    "        q_hat: np.ndarray,\n",
    "        pi_0: np.ndarray,\n",
    "    ) -> tuple:\n",
    "        dataset = GradientBasedPolicyDataset(\n",
    "            torch.from_numpy(x).float(),\n",
    "            torch.from_numpy(a).long(),\n",
    "            torch.from_numpy(r).float(),\n",
    "            torch.from_numpy(pscore).float(),\n",
    "            torch.from_numpy(q_hat).float(),\n",
    "            torch.from_numpy(pi_0).float(),\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def _estimate_policy_gradient(\n",
    "        self,\n",
    "        a: torch.Tensor,\n",
    "        r: torch.Tensor,\n",
    "        pscore: torch.Tensor,\n",
    "        q_hat: torch.Tensor,\n",
    "        pi: torch.Tensor,\n",
    "        pi_0: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        current_pi = pi.detach()\n",
    "        log_prob = torch.log(pi + self.log_eps)\n",
    "        idx = torch.arange(a.shape[0], dtype=torch.long)\n",
    "\n",
    "        q_hat_factual = q_hat[idx, a]\n",
    "        iw = current_pi[idx, a] / pscore\n",
    "        estimated_policy_grad_arr = iw * (r - q_hat_factual) * log_prob[idx, a]\n",
    "        estimated_policy_grad_arr += torch.sum(q_hat * current_pi * log_prob, dim=1)\n",
    "\n",
    "        # imitation regularization\n",
    "        estimated_policy_grad_arr += self.imit_reg * log_prob[idx, a]\n",
    "\n",
    "        return estimated_policy_grad_arr\n",
    "\n",
    "    def _estimate_policy_gradient_2(\n",
    "        self,\n",
    "        a: torch.Tensor,\n",
    "        r: torch.Tensor,\n",
    "        pscore: torch.Tensor,\n",
    "        q_hat: torch.Tensor,\n",
    "        pi: torch.Tensor,\n",
    "        pi_0: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        current_pi = pi.detach()\n",
    "        log_prob = torch.log(pi + self.log_eps)\n",
    "        idx = torch.arange(a.shape[0], dtype=torch.long)\n",
    "\n",
    "        q_hat_factual = q_hat[idx, a]\n",
    "        iw = current_pi[idx, a] / pscore\n",
    "        estimated_policy_grad_arr = iw * (r - q_hat_factual) * log_prob[idx, a]\n",
    "        _estimated_policy_grad_arr = estimated_policy_grad_arr\n",
    "        estimated_policy_grad_arr += torch.sum(q_hat * current_pi * log_prob, dim=1)\n",
    "\n",
    "        # imitation regularization\n",
    "        estimated_policy_grad_arr += self.imit_reg * log_prob[idx, a]\n",
    "\n",
    "        return iw, _estimated_policy_grad_arr\n",
    "    \n",
    "    def predict(self, dataset_test: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        self.nn_model.eval()\n",
    "        x = torch.from_numpy(dataset_test[\"x\"]).float()\n",
    "        return self.nn_model(x).detach().numpy()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class POTEC:\n",
    "    \"\"\"回帰ベースと勾配ベースのアプローチを融合した2段階方策学習\"\"\"\n",
    "    dim_x: int\n",
    "    num_actions: int\n",
    "    num_clusters: int = 1\n",
    "    hidden_layer_size: tuple = (30, 30, 30)\n",
    "    activation: str = \"elu\"\n",
    "    batch_size: int = 16\n",
    "    learning_rate_init: float = 0.005\n",
    "    gamma: float = 0.98\n",
    "    alpha: float = 1e-6\n",
    "    log_eps: float = 1e-10\n",
    "    solver: str = \"adagrad\"\n",
    "    max_iter: int = 30\n",
    "    random_state: int = 12345\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Initialize class.\"\"\"\n",
    "        layer_list = []\n",
    "        input_size = self.dim_x\n",
    "\n",
    "        if self.activation == \"tanh\":\n",
    "            activation_layer = nn.Tanh\n",
    "        elif self.activation == \"relu\":\n",
    "            activation_layer = nn.ReLU\n",
    "        elif self.activation == \"elu\":\n",
    "            activation_layer = nn.ELU\n",
    "\n",
    "        for i, h in enumerate(self.hidden_layer_size):\n",
    "            layer_list.append((\"l{}\".format(i), nn.Linear(input_size, h)))\n",
    "            layer_list.append((\"a{}\".format(i), activation_layer()))\n",
    "            input_size = h\n",
    "        layer_list.append((\"output\", nn.Linear(input_size, self.num_clusters)))\n",
    "        layer_list.append((\"softmax\", nn.Softmax(dim=1)))\n",
    "\n",
    "        self.nn_model = nn.Sequential(OrderedDict(layer_list))\n",
    "\n",
    "        self.random_ = check_random_state(self.random_state)\n",
    "        self.train_loss = []\n",
    "        self.train_value = []\n",
    "        self.test_value = []\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        dataset: dict,\n",
    "        dataset_test: dict,\n",
    "        f_hat: np.ndarray = None,\n",
    "        f_hat_test: np.ndarray = None,\n",
    "    ) -> None:\n",
    "        x, a, r = dataset[\"x\"], dataset[\"a\"], dataset[\"r\"]\n",
    "        pscore_c, phi_a = dataset[\"pscore_c\"], torch.from_numpy(dataset[\"phi_a\"])\n",
    "        if f_hat is None:\n",
    "            f_hat = np.zeros(dataset[\"h_x_a\"].shape)\n",
    "        if f_hat_test is None:\n",
    "            f_hat_test = np.zeros(dataset_test[\"h_x_a\"].shape)\n",
    "\n",
    "        if self.solver == \"adagrad\":\n",
    "            optimizer = optim.Adagrad(\n",
    "                self.nn_model.parameters(),\n",
    "                lr=self.learning_rate_init,\n",
    "                weight_decay=self.alpha,\n",
    "            )\n",
    "        elif self.solver == \"adam\":\n",
    "            optimizer = optim.AdamW(\n",
    "                self.nn_model.parameters(),\n",
    "                lr=self.learning_rate_init,\n",
    "                weight_decay=self.alpha,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"`solver` must be one of 'adam' or 'adagrad'\")\n",
    "\n",
    "        training_data_loader = self._create_train_data_for_opl(\n",
    "            x,\n",
    "            a,\n",
    "            r,\n",
    "            pscore_c,\n",
    "            f_hat,\n",
    "            dataset[\"pi_0_c\"],\n",
    "        )\n",
    "\n",
    "        # start policy training\n",
    "        q_x_a_train, q_x_a_test = dataset[\"q_x_a\"], dataset_test[\"q_x_a\"]\n",
    "        for _ in range(self.max_iter):\n",
    "            loss_epoch = 0.0\n",
    "            self.nn_model.train()\n",
    "            for (x, a, r, p_c, f_hat_, _) in training_data_loader:\n",
    "                optimizer.zero_grad()\n",
    "                pi = self.nn_model(x)\n",
    "                loss = -self._estimate_policy_gradient(\n",
    "                    x=x,\n",
    "                    a=a,\n",
    "                    phi_a=phi_a,\n",
    "                    r=r,\n",
    "                    pscore_c=p_c,\n",
    "                    f_hat=f_hat_,\n",
    "                    pi=pi,\n",
    "                ).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_epoch += loss.item()\n",
    "            self.train_loss.append(loss_epoch)\n",
    "            pi_train = self.predict(dataset, f_hat)\n",
    "            self.train_value.append((q_x_a_train * pi_train).sum(1).mean())\n",
    "            pi_test = self.predict(dataset_test, f_hat_test)\n",
    "            self.test_value.append((q_x_a_test * pi_test).sum(1).mean())\n",
    "\n",
    "    def _create_train_data_for_opl(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        a: np.ndarray,\n",
    "        r: np.ndarray,\n",
    "        pscore_c: np.ndarray,\n",
    "        f_hat: np.ndarray,\n",
    "        pi_0_c: np.ndarray,\n",
    "    ) -> tuple:\n",
    "        dataset = GradientBasedPolicyDataset(\n",
    "            torch.from_numpy(x).float(),\n",
    "            torch.from_numpy(a).long(),\n",
    "            torch.from_numpy(r).float(),\n",
    "            torch.from_numpy(pscore_c).float(),\n",
    "            torch.from_numpy(f_hat).float(),\n",
    "            torch.from_numpy(pi_0_c).float(),\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def _estimate_policy_gradient(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        a: torch.Tensor,\n",
    "        r: torch.Tensor,\n",
    "        phi_a: torch.Tensor,\n",
    "        pscore_c: torch.Tensor,\n",
    "        f_hat: torch.Tensor,\n",
    "        pi: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        current_pi = pi.detach()\n",
    "        log_prob = torch.log(pi + self.log_eps)\n",
    "        idx = torch.arange(a.shape[0], dtype=torch.long)\n",
    "\n",
    "        f_hat_factual = f_hat[idx, a]\n",
    "        iw = current_pi[idx, phi_a[a]] / pscore_c\n",
    "        estimated_policy_grad_arr = iw * (r - f_hat_factual) * log_prob[idx, phi_a[a]]\n",
    "\n",
    "        f_hat_c = torch.zeros((a.shape[0], self.num_clusters))\n",
    "        for c in range(self.num_clusters):\n",
    "            if (phi_a == c).sum() > 0:\n",
    "                f_hat_c[:, c] = f_hat[:, phi_a == c].max(1)[0]\n",
    "            else:\n",
    "                f_hat_c[:, c] = 0.0\n",
    "        estimated_policy_grad_arr += torch.sum(f_hat_c * current_pi * log_prob, dim=1)\n",
    "\n",
    "        return estimated_policy_grad_arr\n",
    "\n",
    "    def predict(self, dataset_test: dict, f_hat_test: np.ndarray) -> np.ndarray:\n",
    "        self.nn_model.eval()\n",
    "        x = torch.from_numpy(dataset_test[\"x\"]).float()\n",
    "        pi_c = self.nn_model(x).detach().numpy()\n",
    "        phi_a = torch.from_numpy(dataset_test[\"phi_a\"])\n",
    "\n",
    "        n = x.shape[0]\n",
    "        action_set = np.arange(f_hat_test.shape[1])\n",
    "        overall_policy = np.zeros(f_hat_test.shape)\n",
    "        for c in range(self.num_clusters):\n",
    "            if (phi_a == c).sum() > 0:\n",
    "                best_actions_given_clusters = action_set[phi_a == c][\n",
    "                    f_hat_test[:, phi_a == c].argmax(1)\n",
    "                ]\n",
    "                overall_policy[np.arange(n), best_actions_given_clusters] = pi_c[:, c]\n",
    "\n",
    "        return overall_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d27dcc4-042a-4259-997d-9e67c7ccaf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>a</th>\n",
       "      <th>c</th>\n",
       "      <th>r</th>\n",
       "      <th>pi_0_0</th>\n",
       "      <th>pi_0_1</th>\n",
       "      <th>pi_0_2</th>\n",
       "      <th>pi_0_c_0</th>\n",
       "      <th>...</th>\n",
       "      <th>pscore</th>\n",
       "      <th>pscore_c</th>\n",
       "      <th>g_x_c_0</th>\n",
       "      <th>g_x_c_1</th>\n",
       "      <th>h_x_a_0</th>\n",
       "      <th>h_x_a_1</th>\n",
       "      <th>h_x_a_2</th>\n",
       "      <th>q_x_a_0</th>\n",
       "      <th>q_x_a_1</th>\n",
       "      <th>q_x_a_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.764052</td>\n",
       "      <td>0.400157</td>\n",
       "      <td>0.978738</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.330557</td>\n",
       "      <td>0.438512</td>\n",
       "      <td>0.230930</td>\n",
       "      <td>0.330557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438512</td>\n",
       "      <td>0.669443</td>\n",
       "      <td>0.499947</td>\n",
       "      <td>0.012539</td>\n",
       "      <td>0.011047</td>\n",
       "      <td>0.380021</td>\n",
       "      <td>0.095861</td>\n",
       "      <td>0.510994</td>\n",
       "      <td>0.392559</td>\n",
       "      <td>0.108399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.240893</td>\n",
       "      <td>1.867558</td>\n",
       "      <td>-0.977278</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.154751</td>\n",
       "      <td>0.483056</td>\n",
       "      <td>0.362193</td>\n",
       "      <td>0.154751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362193</td>\n",
       "      <td>0.845249</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.244939</td>\n",
       "      <td>0.499999</td>\n",
       "      <td>0.496463</td>\n",
       "      <td>0.744939</td>\n",
       "      <td>0.500066</td>\n",
       "      <td>0.496530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.950088</td>\n",
       "      <td>-0.151357</td>\n",
       "      <td>-0.103219</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.292559</td>\n",
       "      <td>0.309976</td>\n",
       "      <td>0.397465</td>\n",
       "      <td>0.292559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397465</td>\n",
       "      <td>0.707441</td>\n",
       "      <td>0.377560</td>\n",
       "      <td>0.277070</td>\n",
       "      <td>0.378410</td>\n",
       "      <td>0.405722</td>\n",
       "      <td>0.415666</td>\n",
       "      <td>0.755970</td>\n",
       "      <td>0.682791</td>\n",
       "      <td>0.692736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.410599</td>\n",
       "      <td>0.144044</td>\n",
       "      <td>1.454274</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.491739</td>\n",
       "      <td>0.376881</td>\n",
       "      <td>0.131380</td>\n",
       "      <td>0.491739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376881</td>\n",
       "      <td>0.508261</td>\n",
       "      <td>0.492283</td>\n",
       "      <td>0.396090</td>\n",
       "      <td>0.499219</td>\n",
       "      <td>0.302622</td>\n",
       "      <td>0.498354</td>\n",
       "      <td>0.991502</td>\n",
       "      <td>0.698712</td>\n",
       "      <td>0.894444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.761038</td>\n",
       "      <td>0.121675</td>\n",
       "      <td>0.443863</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.254738</td>\n",
       "      <td>0.260212</td>\n",
       "      <td>0.485050</td>\n",
       "      <td>0.254738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260212</td>\n",
       "      <td>0.745262</td>\n",
       "      <td>0.228768</td>\n",
       "      <td>0.356311</td>\n",
       "      <td>0.328859</td>\n",
       "      <td>0.289922</td>\n",
       "      <td>0.411425</td>\n",
       "      <td>0.557627</td>\n",
       "      <td>0.646232</td>\n",
       "      <td>0.767735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>-0.484180</td>\n",
       "      <td>0.207029</td>\n",
       "      <td>0.754987</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.319816</td>\n",
       "      <td>0.339420</td>\n",
       "      <td>0.340763</td>\n",
       "      <td>0.319816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340763</td>\n",
       "      <td>0.680184</td>\n",
       "      <td>0.323749</td>\n",
       "      <td>0.208238</td>\n",
       "      <td>0.295656</td>\n",
       "      <td>0.174609</td>\n",
       "      <td>0.391237</td>\n",
       "      <td>0.619405</td>\n",
       "      <td>0.382847</td>\n",
       "      <td>0.599474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1.359420</td>\n",
       "      <td>0.696072</td>\n",
       "      <td>0.682201</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.438499</td>\n",
       "      <td>0.322525</td>\n",
       "      <td>0.238976</td>\n",
       "      <td>0.438499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322525</td>\n",
       "      <td>0.561501</td>\n",
       "      <td>0.487573</td>\n",
       "      <td>0.140192</td>\n",
       "      <td>0.109516</td>\n",
       "      <td>0.335420</td>\n",
       "      <td>0.316686</td>\n",
       "      <td>0.597089</td>\n",
       "      <td>0.475613</td>\n",
       "      <td>0.456878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>-0.011214</td>\n",
       "      <td>1.344760</td>\n",
       "      <td>-0.831492</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.410658</td>\n",
       "      <td>0.359443</td>\n",
       "      <td>0.229899</td>\n",
       "      <td>0.410658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359443</td>\n",
       "      <td>0.589342</td>\n",
       "      <td>0.485086</td>\n",
       "      <td>0.270239</td>\n",
       "      <td>0.499298</td>\n",
       "      <td>0.498875</td>\n",
       "      <td>0.499573</td>\n",
       "      <td>0.984384</td>\n",
       "      <td>0.769114</td>\n",
       "      <td>0.769812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>-0.407972</td>\n",
       "      <td>-1.330804</td>\n",
       "      <td>0.352599</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.468760</td>\n",
       "      <td>0.269751</td>\n",
       "      <td>0.261489</td>\n",
       "      <td>0.468760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269751</td>\n",
       "      <td>0.531240</td>\n",
       "      <td>0.347049</td>\n",
       "      <td>0.065895</td>\n",
       "      <td>0.416049</td>\n",
       "      <td>0.399401</td>\n",
       "      <td>0.037246</td>\n",
       "      <td>0.763098</td>\n",
       "      <td>0.465296</td>\n",
       "      <td>0.103141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-0.537885</td>\n",
       "      <td>0.393444</td>\n",
       "      <td>0.286518</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.420900</td>\n",
       "      <td>0.200825</td>\n",
       "      <td>0.378275</td>\n",
       "      <td>0.420900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200825</td>\n",
       "      <td>0.579100</td>\n",
       "      <td>0.267451</td>\n",
       "      <td>0.184281</td>\n",
       "      <td>0.212071</td>\n",
       "      <td>0.180569</td>\n",
       "      <td>0.352700</td>\n",
       "      <td>0.479522</td>\n",
       "      <td>0.364849</td>\n",
       "      <td>0.536980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x_0       x_1       x_2  a  c  r    pi_0_0    pi_0_1    pi_0_2  \\\n",
       "0     1.764052  0.400157  0.978738  1  1  0  0.330557  0.438512  0.230930   \n",
       "1     2.240893  1.867558 -0.977278  2  1  1  0.154751  0.483056  0.362193   \n",
       "2     0.950088 -0.151357 -0.103219  2  1  0  0.292559  0.309976  0.397465   \n",
       "3     0.410599  0.144044  1.454274  1  1  1  0.491739  0.376881  0.131380   \n",
       "4     0.761038  0.121675  0.443863  1  1  1  0.254738  0.260212  0.485050   \n",
       "...        ...       ...       ... .. .. ..       ...       ...       ...   \n",
       "1995 -0.484180  0.207029  0.754987  2  1  1  0.319816  0.339420  0.340763   \n",
       "1996  1.359420  0.696072  0.682201  1  1  0  0.438499  0.322525  0.238976   \n",
       "1997 -0.011214  1.344760 -0.831492  1  1  1  0.410658  0.359443  0.229899   \n",
       "1998 -0.407972 -1.330804  0.352599  1  1  0  0.468760  0.269751  0.261489   \n",
       "1999 -0.537885  0.393444  0.286518  1  1  1  0.420900  0.200825  0.378275   \n",
       "\n",
       "      pi_0_c_0  ...    pscore  pscore_c   g_x_c_0   g_x_c_1   h_x_a_0  \\\n",
       "0     0.330557  ...  0.438512  0.669443  0.499947  0.012539  0.011047   \n",
       "1     0.154751  ...  0.362193  0.845249  0.500000  0.000067  0.244939   \n",
       "2     0.292559  ...  0.397465  0.707441  0.377560  0.277070  0.378410   \n",
       "3     0.491739  ...  0.376881  0.508261  0.492283  0.396090  0.499219   \n",
       "4     0.254738  ...  0.260212  0.745262  0.228768  0.356311  0.328859   \n",
       "...        ...  ...       ...       ...       ...       ...       ...   \n",
       "1995  0.319816  ...  0.340763  0.680184  0.323749  0.208238  0.295656   \n",
       "1996  0.438499  ...  0.322525  0.561501  0.487573  0.140192  0.109516   \n",
       "1997  0.410658  ...  0.359443  0.589342  0.485086  0.270239  0.499298   \n",
       "1998  0.468760  ...  0.269751  0.531240  0.347049  0.065895  0.416049   \n",
       "1999  0.420900  ...  0.200825  0.579100  0.267451  0.184281  0.212071   \n",
       "\n",
       "       h_x_a_1   h_x_a_2   q_x_a_0   q_x_a_1   q_x_a_2  \n",
       "0     0.380021  0.095861  0.510994  0.392559  0.108399  \n",
       "1     0.499999  0.496463  0.744939  0.500066  0.496530  \n",
       "2     0.405722  0.415666  0.755970  0.682791  0.692736  \n",
       "3     0.302622  0.498354  0.991502  0.698712  0.894444  \n",
       "4     0.289922  0.411425  0.557627  0.646232  0.767735  \n",
       "...        ...       ...       ...       ...       ...  \n",
       "1995  0.174609  0.391237  0.619405  0.382847  0.599474  \n",
       "1996  0.335420  0.316686  0.597089  0.475613  0.456878  \n",
       "1997  0.498875  0.499573  0.984384  0.769114  0.769812  \n",
       "1998  0.399401  0.037246  0.763098  0.465296  0.103141  \n",
       "1999  0.180569  0.352700  0.479522  0.364849  0.536980  \n",
       "\n",
       "[2000 rows x 21 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## シミュレーション設定\n",
    "num_runs = 100 # シミュレーションの繰り返し回数\n",
    "dim_x = 3 # 特徴量xの次元\n",
    "num_actions = 3 # 行動数, |A|\n",
    "num_clusters = 2 # 行動クラスタ数, |C|\n",
    "lambda_ = 0.5 # クラスタ効果と残差効果の配合率\n",
    "max_iter = 31 # エポック数\n",
    "test_data_size = 50000 # テストデータのサイズ\n",
    "random_state = 12345\n",
    "torch.manual_seed(random_state)\n",
    "random_ = check_random_state(random_state)\n",
    "num_data_list = [100, 200, 500, 1000, 2000] # トレーニングデータのサイズ\n",
    "num_data = num_data_list[4]\n",
    "\n",
    "## 期待報酬関数を定義するためのパラメータを抽出\n",
    "phi_a = random_.choice(num_clusters, size=num_actions)\n",
    "theta_g = random_.normal(size=(dim_x, num_clusters))\n",
    "M_g = random_.normal(size=(dim_x, num_clusters))\n",
    "b_g = random_.normal(size=(1, num_clusters))\n",
    "theta_h = random_.normal(size=(dim_x, num_actions))\n",
    "M_h = random_.normal(size=(dim_x, num_actions))\n",
    "b_h = random_.normal(size=(1, num_actions))\n",
    "\n",
    "## 学習された方策の真の性能を近似するためのテストデータを生成\n",
    "test_data = generate_synthetic_data(\n",
    "    num_data=test_data_size, lambda_=lambda_,\n",
    "    theta_g=theta_g, M_g=M_g, b_g=b_g, theta_h=theta_h, M_h=M_h, b_h=b_h, phi_a=phi_a,\n",
    "    dim_context=dim_x, num_actions=num_actions, num_clusters=num_clusters, random_state = random_state\n",
    ")\n",
    "pi_0_value = (test_data[\"q_x_a\"] * test_data[\"pi_0\"]).sum(1).mean()\n",
    "\n",
    "_ = 0\n",
    "offline_logged_data = generate_synthetic_data(\n",
    "    num_data=num_data, lambda_=lambda_,\n",
    "    theta_g=theta_g, M_g=M_g, b_g=b_g, theta_h=theta_h, M_h=M_h, b_h=b_h, phi_a=phi_a,\n",
    "    dim_context=dim_x, num_actions=num_actions, num_clusters=num_clusters,\n",
    "    random_state = _\n",
    ")\n",
    "x_df = pd.DataFrame(offline_logged_data['x'], columns=['x_0', 'x_1', 'x_2'])\n",
    "a_df = pd.DataFrame(offline_logged_data['a'], columns=['a'])\n",
    "c_df = pd.DataFrame(offline_logged_data['c'], columns=['c'])\n",
    "r_df = pd.DataFrame(offline_logged_data['r'], columns=['r'])\n",
    "pi_0_df = pd.DataFrame(offline_logged_data['pi_0'], columns=['pi_0_0', 'pi_0_1', 'pi_0_2'])\n",
    "pi_0_c_df = pd.DataFrame(offline_logged_data['pi_0_c'], columns=['pi_0_c_0', 'pi_0_c_1'])\n",
    "pscore_df = pd.DataFrame(offline_logged_data['pscore'], columns=['pscore'])\n",
    "pscore_c_df = pd.DataFrame(offline_logged_data['pscore_c'], columns=['pscore_c'])\n",
    "g_x_c_df = pd.DataFrame(offline_logged_data['g_x_c'], columns=['g_x_c_0', 'g_x_c_1'])\n",
    "h_x_a_df = pd.DataFrame(offline_logged_data['h_x_a'], columns=['h_x_a_0', 'h_x_a_1', 'h_x_a_2'])\n",
    "q_x_a_df = pd.DataFrame(offline_logged_data['q_x_a'], columns=['q_x_a_0', 'q_x_a_1', 'q_x_a_2'])\n",
    "\n",
    "offline_logged_data_df = pd.concat([\n",
    "    x_df, a_df, c_df, r_df, pi_0_df, pi_0_c_df, pscore_df, pscore_c_df, g_x_c_df, h_x_a_df, q_x_a_df\n",
    "], axis=1)\n",
    "offline_logged_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a873e2ec-aaf0-45f2-9fcd-2264dc4e1597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "報酬実績値: 0.635\n"
     ]
    }
   ],
   "source": [
    "# 報酬実績値\n",
    "result_r = offline_logged_data_df['r'].mean()\n",
    "print(f'報酬実績値: {result_r:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c251f720-b590-425c-9898-b8d079797cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回帰ベースのアプローチによる報酬値: 0.746\n"
     ]
    }
   ],
   "source": [
    "### 回帰ベースのアプローチ\n",
    "reg = RegBasedPolicyLearner(dim_x=dim_x, num_actions=num_actions, max_iter=max_iter)\n",
    "reg.fit(offline_logged_data, test_data)\n",
    "pi_reg = reg.predict(test_data)\n",
    "true_value_of_learned_policies_reg = (test_data[\"q_x_a\"] * pi_reg).sum(1).mean()\n",
    "print(f'回帰ベースのアプローチによる報酬値: {true_value_of_learned_policies_reg:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bc468c7-6573-4b32-84e5-c7e47d818bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offline_logged_data['phi_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d1c32-2724-42b4-9671-5703b528b4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
