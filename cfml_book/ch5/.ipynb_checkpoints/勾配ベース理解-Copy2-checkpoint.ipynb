{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624fc680-a0b8-4ae8-957d-d0c419f3d4dc",
   "metadata": {},
   "source": [
    "# 勾配ベース理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d04e387-83d3-4b40-bd8e-461a413d577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import check_random_state\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from dataset import generate_synthetic_data\n",
    "# from policylearners import RegBasedPolicyLearner, GradientBasedPolicyLearner, POTEC\n",
    "from utils import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b5d590-d871-4657-a025-e222054db93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## シミュレーション設定\n",
    "num_runs = 100 # シミュレーションの繰り返し回数\n",
    "dim_x = 5 # 特徴量xの次元\n",
    "num_actions = 2 # 行動数, |A|\n",
    "num_clusters = 2 # 行動クラスタ数, |C|\n",
    "lambda_ = 0.5 # クラスタ効果と残差効果の配合率\n",
    "max_iter = 31 # エポック数\n",
    "test_data_size = 50000 # テストデータのサイズ\n",
    "random_state = 12345\n",
    "torch.manual_seed(random_state)\n",
    "random_ = check_random_state(random_state)\n",
    "num_data_list = [100, 200, 500, 1000, 2000] # トレーニングデータのサイズ\n",
    "\n",
    "# num_data = num_data_list[4] # 2,000\n",
    "num_data = num_data_list[0] # 100\n",
    "\n",
    "## 期待報酬関数を定義するためのパラメータを抽出\n",
    "phi_a = random_.choice(num_clusters, size=num_actions)\n",
    "theta_g = random_.normal(size=(dim_x, num_clusters))\n",
    "M_g = random_.normal(size=(dim_x, num_clusters))\n",
    "b_g = random_.normal(size=(1, num_clusters))\n",
    "theta_h = random_.normal(size=(dim_x, num_actions))\n",
    "M_h = random_.normal(size=(dim_x, num_actions))\n",
    "b_h = random_.normal(size=(1, num_actions))\n",
    "\n",
    "## 学習された方策の真の性能を近似するためのテストデータを生成\n",
    "test_data = generate_synthetic_data(\n",
    "    num_data=test_data_size, lambda_=lambda_,\n",
    "    theta_g=theta_g, M_g=M_g, b_g=b_g, theta_h=theta_h, M_h=M_h, b_h=b_h, phi_a=phi_a,\n",
    "    dim_context=dim_x, num_actions=num_actions, num_clusters=num_clusters, random_state = random_state\n",
    ")\n",
    "pi_0_value = (test_data[\"q_x_a\"] * test_data[\"pi_0\"]).sum(1).mean()\n",
    "\n",
    "## データ収集方策が形成する分布に従いログデータを生成\n",
    "offline_logged_data = generate_synthetic_data(\n",
    "    num_data=num_data, lambda_=lambda_,\n",
    "    theta_g=theta_g, M_g=M_g, b_g=b_g, theta_h=theta_h, M_h=M_h, b_h=b_h, phi_a=phi_a,\n",
    "    dim_context=dim_x, num_actions=num_actions, num_clusters=num_clusters,\n",
    "    # random_state = _\n",
    "    random_state = random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d210fcd-ca3a-4995-a12d-59da170e4f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>a</th>\n",
       "      <th>r</th>\n",
       "      <th>q_x_0</th>\n",
       "      <th>q_x_1</th>\n",
       "      <th>pi_0_0</th>\n",
       "      <th>pi_0_1</th>\n",
       "      <th>pscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.204708</td>\n",
       "      <td>0.478943</td>\n",
       "      <td>-0.519439</td>\n",
       "      <td>-0.555730</td>\n",
       "      <td>1.965781</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.175144</td>\n",
       "      <td>0.042578</td>\n",
       "      <td>0.431547</td>\n",
       "      <td>0.568453</td>\n",
       "      <td>0.568453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.393406</td>\n",
       "      <td>0.092908</td>\n",
       "      <td>0.281746</td>\n",
       "      <td>0.769023</td>\n",
       "      <td>1.246435</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.357402</td>\n",
       "      <td>0.026896</td>\n",
       "      <td>0.545722</td>\n",
       "      <td>0.454278</td>\n",
       "      <td>0.545722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.007189</td>\n",
       "      <td>-1.296221</td>\n",
       "      <td>0.274992</td>\n",
       "      <td>0.228913</td>\n",
       "      <td>1.352917</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086599</td>\n",
       "      <td>0.165448</td>\n",
       "      <td>0.561380</td>\n",
       "      <td>0.438620</td>\n",
       "      <td>0.561380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.886429</td>\n",
       "      <td>-2.001637</td>\n",
       "      <td>-0.371843</td>\n",
       "      <td>1.669025</td>\n",
       "      <td>-0.438570</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.777752</td>\n",
       "      <td>0.381532</td>\n",
       "      <td>0.618468</td>\n",
       "      <td>0.381532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.539741</td>\n",
       "      <td>0.476985</td>\n",
       "      <td>3.248944</td>\n",
       "      <td>-1.021228</td>\n",
       "      <td>-0.577087</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.440605</td>\n",
       "      <td>0.559395</td>\n",
       "      <td>0.559395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.398092</td>\n",
       "      <td>-0.916935</td>\n",
       "      <td>-0.082650</td>\n",
       "      <td>-1.939691</td>\n",
       "      <td>1.407994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.456561</td>\n",
       "      <td>0.442579</td>\n",
       "      <td>0.557421</td>\n",
       "      <td>0.557421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.512406</td>\n",
       "      <td>0.526493</td>\n",
       "      <td>-0.266931</td>\n",
       "      <td>0.862284</td>\n",
       "      <td>0.083803</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.728565</td>\n",
       "      <td>0.313117</td>\n",
       "      <td>0.620969</td>\n",
       "      <td>0.379031</td>\n",
       "      <td>0.620969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-1.872339</td>\n",
       "      <td>-0.962791</td>\n",
       "      <td>0.080067</td>\n",
       "      <td>0.128726</td>\n",
       "      <td>-0.479120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.728029</td>\n",
       "      <td>0.998228</td>\n",
       "      <td>0.442754</td>\n",
       "      <td>0.557246</td>\n",
       "      <td>0.557246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.640281</td>\n",
       "      <td>0.745974</td>\n",
       "      <td>-0.622547</td>\n",
       "      <td>0.936289</td>\n",
       "      <td>0.750018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.920900</td>\n",
       "      <td>0.913424</td>\n",
       "      <td>0.549833</td>\n",
       "      <td>0.450167</td>\n",
       "      <td>0.450167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.056715</td>\n",
       "      <td>2.300675</td>\n",
       "      <td>0.569497</td>\n",
       "      <td>1.489410</td>\n",
       "      <td>1.264250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999960</td>\n",
       "      <td>0.627659</td>\n",
       "      <td>0.372341</td>\n",
       "      <td>0.627659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x_0       x_1       x_2       x_3       x_4  a  r     q_x_0  \\\n",
       "0  -0.204708  0.478943 -0.519439 -0.555730  1.965781  1  0  0.175144   \n",
       "1   1.393406  0.092908  0.281746  0.769023  1.246435  0  0  0.357402   \n",
       "2   1.007189 -1.296221  0.274992  0.228913  1.352917  0  0  0.086599   \n",
       "3   0.886429 -2.001637 -0.371843  1.669025 -0.438570  0  0  0.000486   \n",
       "4  -0.539741  0.476985  3.248944 -1.021228 -0.577087  1  1  1.000000   \n",
       "..       ...       ...       ...       ...       ... .. ..       ...   \n",
       "95  0.398092 -0.916935 -0.082650 -1.939691  1.407994  1  1  0.000950   \n",
       "96  1.512406  0.526493 -0.266931  0.862284  0.083803  0  1  0.728565   \n",
       "97 -1.872339 -0.962791  0.080067  0.128726 -0.479120  1  1  0.728029   \n",
       "98 -0.640281  0.745974 -0.622547  0.936289  0.750018  1  1  0.920900   \n",
       "99 -0.056715  2.300675  0.569497  1.489410  1.264250  0  1  0.999998   \n",
       "\n",
       "       q_x_1    pi_0_0    pi_0_1    pscore  \n",
       "0   0.042578  0.431547  0.568453  0.568453  \n",
       "1   0.026896  0.545722  0.454278  0.545722  \n",
       "2   0.165448  0.561380  0.438620  0.561380  \n",
       "3   0.777752  0.381532  0.618468  0.381532  \n",
       "4   1.000000  0.440605  0.559395  0.559395  \n",
       "..       ...       ...       ...       ...  \n",
       "95  0.456561  0.442579  0.557421  0.557421  \n",
       "96  0.313117  0.620969  0.379031  0.620969  \n",
       "97  0.998228  0.442754  0.557246  0.557246  \n",
       "98  0.913424  0.549833  0.450167  0.450167  \n",
       "99  0.999960  0.627659  0.372341  0.627659  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 辞書型で直観的にわかりづらいのでdfに変換\n",
    "# log_data->log_data_df\n",
    "data = offline_logged_data.copy()\n",
    "df = pd.DataFrame(data['x'], columns=['x_'+str(i) for i in range(data['x'].shape[1])])\n",
    "df['a'] = data['a']\n",
    "df['r'] = data['r']\n",
    "ex_reward_df = pd.DataFrame(data['q_x_a'], columns=['q_x_'+str(i) for i in range(data['num_actions'])])\n",
    "df = pd.concat([df, ex_reward_df], axis=1)\n",
    "pi_b_df = pd.DataFrame(data['pi_0'].reshape(data['num_data'], data['num_actions']), columns=['pi_0_'+str(i) for i in range(data['num_actions'])])\n",
    "df = pd.concat([df, pi_b_df], axis=1)\n",
    "df['pscore'] = data['pscore']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd12719c-b5a2-420b-aa4a-effcc8f9c0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('offline_logged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b575e3c-6fdb-4778-97e9-2f4b8c4f07a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from sklearn.utils import check_random_state\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from utils import softmax, RegBasedPolicyDataset, GradientBasedPolicyDataset\n",
    "\n",
    "# class\n",
    "dim_x: int = dim_x\n",
    "num_actions: int = num_actions\n",
    "hidden_layer_size: tuple = (30, 30, 30)\n",
    "activation: str = \"elu\"\n",
    "batch_size: int = 10\n",
    "learning_rate_init: float = 0.005\n",
    "gamma: float = 0.98\n",
    "alpha: float = 1e-6\n",
    "imit_reg: float = 0.0\n",
    "log_eps: float = 1e-10\n",
    "solver: str = \"adagrad\"\n",
    "max_iter: int = 30\n",
    "random_state: int = 12345\n",
    "\n",
    "# init\n",
    "layer_list = []\n",
    "input_size = dim_x\n",
    "\n",
    "if activation == \"tanh\":\n",
    "    activation_layer = nn.Tanh\n",
    "elif activation == \"relu\":\n",
    "    activation_layer = nn.ReLU\n",
    "elif activation == \"elu\":\n",
    "    activation_layer = nn.ELU\n",
    "\n",
    "for i, h in enumerate(hidden_layer_size):\n",
    "    layer_list.append((\"l{}\".format(i), nn.Linear(input_size, h)))\n",
    "    layer_list.append((\"a{}\".format(i), activation_layer()))\n",
    "    input_size = h\n",
    "layer_list.append((\"output\", nn.Linear(input_size, num_actions)))\n",
    "\n",
    "nn_model = nn.Sequential(OrderedDict(layer_list))\n",
    "\n",
    "random_ = check_random_state(random_state)\n",
    "train_loss = []\n",
    "train_value = []\n",
    "test_value = []\n",
    "\n",
    "# _create_train_data_for_opl\n",
    "def _create_train_data_for_opl(\n",
    "    x: np.ndarray,\n",
    "    a: np.ndarray,\n",
    "    r: np.ndarray,\n",
    "    pscore: np.ndarray,\n",
    "    q_hat: np.ndarray,\n",
    "    pi_0: np.ndarray,\n",
    ") -> tuple:\n",
    "    dataset = GradientBasedPolicyDataset(\n",
    "        torch.from_numpy(x).float(),\n",
    "        torch.from_numpy(a).long(),\n",
    "        torch.from_numpy(r).float(),\n",
    "        torch.from_numpy(pscore).float(),\n",
    "        torch.from_numpy(q_hat).float(),\n",
    "        torch.from_numpy(pi_0).float(),\n",
    "    )\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "# _estimate_policy_gradient\n",
    "def _estimate_policy_gradient(\n",
    "    a: torch.Tensor,\n",
    "    r: torch.Tensor,\n",
    "    pscore: torch.Tensor,\n",
    "    q_hat: torch.Tensor,\n",
    "    pi: torch.Tensor,\n",
    "    pi_0: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    current_pi = pi.detach()\n",
    "    log_prob = torch.log(pi + log_eps)\n",
    "    idx = torch.arange(a.shape[0], dtype=torch.long)\n",
    "\n",
    "    q_hat_factual = q_hat[idx, a]\n",
    "    iw = current_pi[idx, a] / pscore\n",
    "    estimated_policy_grad_arr = iw * (r - q_hat_factual) * log_prob[idx, a]\n",
    "    estimated_policy_grad_arr += torch.sum(q_hat * current_pi * log_prob, dim=1)\n",
    "\n",
    "    # imitation regularization\n",
    "    estimated_policy_grad_arr += imit_reg * log_prob[idx, a]\n",
    "\n",
    "    return estimated_policy_grad_arr\n",
    "\n",
    "# predict\n",
    "def predict(dataset_test: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    nn_model.eval()\n",
    "    x = torch.from_numpy(dataset_test[\"x\"]).float()\n",
    "    return nn_model(x).detach().numpy()\n",
    "\n",
    "# fit\n",
    "dataset = offline_logged_data\n",
    "dataset_test = test_data\n",
    "q_hat = None\n",
    "\n",
    "x, a, r = dataset[\"x\"], dataset[\"a\"], dataset[\"r\"]\n",
    "pscore, pi_0 = dataset[\"pscore\"], dataset[\"pi_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bd8cd01-71d2-4694-897f-f7e32697dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if q_hat is None:\n",
    "    q_hat = np.zeros((r.shape[0], num_actions))\n",
    "\n",
    "if solver == \"adagrad\":\n",
    "    optimizer = optim.Adagrad(\n",
    "        nn_model.parameters(),\n",
    "        lr=learning_rate_init,\n",
    "        weight_decay=alpha,\n",
    "    )\n",
    "elif solver == \"adam\":\n",
    "    optimizer = optim.AdamW(\n",
    "        nn_model.parameters(),\n",
    "        lr=learning_rate_init,\n",
    "        weight_decay=alpha,\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError(\"`solver` must be one of 'adam' or 'adagrad'\")\n",
    "\n",
    "training_data_loader = _create_train_data_for_opl(\n",
    "    x,\n",
    "    a,\n",
    "    r,\n",
    "    pscore,\n",
    "    q_hat,\n",
    "    pi_0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cb2bd2c-cef4-4d62-b4d9-ed8e7b353566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (l0): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (a0): ELU(alpha=1.0)\n",
      "  (l1): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a1): ELU(alpha=1.0)\n",
      "  (l2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a2): ELU(alpha=1.0)\n",
      "  (output): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "tensor([[1.0508, 1.6958],\n",
      "        [0.8254, 1.0537],\n",
      "        [1.0413, 1.4871],\n",
      "        [0.7560, 0.9081],\n",
      "        [0.8688, 1.3850],\n",
      "        [0.9912, 1.4644],\n",
      "        [0.7446, 1.4035],\n",
      "        [0.9548, 1.5378],\n",
      "        [0.7649, 1.3943],\n",
      "        [0.8751, 1.2964]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.3537, grad_fn=<NegBackward0>)\n",
      "==============================\n",
      "Sequential(\n",
      "  (l0): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (a0): ELU(alpha=1.0)\n",
      "  (l1): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a1): ELU(alpha=1.0)\n",
      "  (l2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a2): ELU(alpha=1.0)\n",
      "  (output): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "tensor([[0.9176, 1.2740],\n",
      "        [0.9131, 1.3728],\n",
      "        [0.8245, 1.1158],\n",
      "        [0.6697, 1.1775],\n",
      "        [0.8994, 1.3345],\n",
      "        [0.9271, 1.5830],\n",
      "        [1.1222, 1.6795],\n",
      "        [0.9283, 1.3328],\n",
      "        [0.4992, 0.5214],\n",
      "        [0.6667, 0.9070]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.4745, grad_fn=<NegBackward0>)\n",
      "==============================\n",
      "Sequential(\n",
      "  (l0): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (a0): ELU(alpha=1.0)\n",
      "  (l1): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a1): ELU(alpha=1.0)\n",
      "  (l2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a2): ELU(alpha=1.0)\n",
      "  (output): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "tensor([[1.0165, 1.8148],\n",
      "        [1.0769, 1.7082],\n",
      "        [0.7828, 1.4107],\n",
      "        [0.5507, 0.7290],\n",
      "        [1.1264, 1.8154],\n",
      "        [0.8594, 1.3910],\n",
      "        [1.3453, 2.0905],\n",
      "        [0.7212, 1.1207],\n",
      "        [0.8644, 1.4236],\n",
      "        [1.0220, 1.5075]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.8542, grad_fn=<NegBackward0>)\n",
      "==============================\n",
      "Sequential(\n",
      "  (l0): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (a0): ELU(alpha=1.0)\n",
      "  (l1): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a1): ELU(alpha=1.0)\n",
      "  (l2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a2): ELU(alpha=1.0)\n",
      "  (output): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "tensor([[1.0903, 1.9947],\n",
      "        [1.0679, 1.6061],\n",
      "        [0.6544, 0.8246],\n",
      "        [0.8674, 0.9925],\n",
      "        [0.5927, 0.8703],\n",
      "        [0.8983, 1.3770],\n",
      "        [1.0374, 1.4536],\n",
      "        [0.7817, 1.2013],\n",
      "        [1.2775, 2.0424],\n",
      "        [0.7947, 1.0728]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.5378, grad_fn=<NegBackward0>)\n",
      "==============================\n",
      "Sequential(\n",
      "  (l0): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (a0): ELU(alpha=1.0)\n",
      "  (l1): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a1): ELU(alpha=1.0)\n",
      "  (l2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a2): ELU(alpha=1.0)\n",
      "  (output): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "tensor([[0.6880, 1.0787],\n",
      "        [1.3383, 2.3946],\n",
      "        [0.6230, 0.7780],\n",
      "        [0.8421, 1.0944],\n",
      "        [1.0890, 2.0561],\n",
      "        [0.7190, 1.1014],\n",
      "        [1.0119, 1.5514],\n",
      "        [1.3202, 2.3287],\n",
      "        [1.3585, 2.3781],\n",
      "        [0.8995, 1.3930]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.7571, grad_fn=<NegBackward0>)\n",
      "==============================\n",
      "Sequential(\n",
      "  (l0): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (a0): ELU(alpha=1.0)\n",
      "  (l1): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a1): ELU(alpha=1.0)\n",
      "  (l2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a2): ELU(alpha=1.0)\n",
      "  (output): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "tensor([[1.0654, 1.7377],\n",
      "        [1.2816, 2.3424],\n",
      "        [0.7450, 0.9346],\n",
      "        [1.2929, 2.2061],\n",
      "        [0.8645, 1.4245],\n",
      "        [1.0709, 1.7866],\n",
      "        [1.2917, 2.2156],\n",
      "        [1.6329, 2.8779],\n",
      "        [0.9473, 1.4736],\n",
      "        [1.0701, 1.7194]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-1.5033, grad_fn=<NegBackward0>)\n",
      "==============================\n",
      "Sequential(\n",
      "  (l0): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (a0): ELU(alpha=1.0)\n",
      "  (l1): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a1): ELU(alpha=1.0)\n",
      "  (l2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a2): ELU(alpha=1.0)\n",
      "  (output): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "tensor([[1.0862, 1.6696],\n",
      "        [0.7899, 1.4415],\n",
      "        [0.8165, 1.1561],\n",
      "        [1.1808, 1.6686],\n",
      "        [0.9289, 1.4533],\n",
      "        [0.6892, 1.1141],\n",
      "        [0.9490, 1.2597],\n",
      "        [1.1775, 1.8933],\n",
      "        [0.9912, 1.3668],\n",
      "        [0.5533, 0.7525]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.2665, grad_fn=<NegBackward0>)\n",
      "==============================\n",
      "Sequential(\n",
      "  (l0): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (a0): ELU(alpha=1.0)\n",
      "  (l1): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a1): ELU(alpha=1.0)\n",
      "  (l2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a2): ELU(alpha=1.0)\n",
      "  (output): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "tensor([[1.6450, 2.5727],\n",
      "        [1.2650, 2.0806],\n",
      "        [1.1093, 1.8425],\n",
      "        [1.0526, 1.4686],\n",
      "        [1.6141, 2.6977],\n",
      "        [1.7840, 3.0709],\n",
      "        [1.4841, 2.5054],\n",
      "        [1.0181, 1.7941],\n",
      "        [0.5874, 0.8262],\n",
      "        [1.4367, 2.2912]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-1.4645, grad_fn=<NegBackward0>)\n",
      "==============================\n",
      "Sequential(\n",
      "  (l0): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (a0): ELU(alpha=1.0)\n",
      "  (l1): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a1): ELU(alpha=1.0)\n",
      "  (l2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a2): ELU(alpha=1.0)\n",
      "  (output): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "tensor([[1.3197, 2.0329],\n",
      "        [0.9529, 1.5155],\n",
      "        [0.3054, 0.3848],\n",
      "        [1.4070, 2.5027],\n",
      "        [1.4300, 2.2790],\n",
      "        [0.5098, 0.7934],\n",
      "        [1.0022, 1.4908],\n",
      "        [1.7161, 2.6428],\n",
      "        [0.8263, 1.1736],\n",
      "        [1.5700, 2.8354]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0393, grad_fn=<NegBackward0>)\n",
      "==============================\n",
      "Sequential(\n",
      "  (l0): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (a0): ELU(alpha=1.0)\n",
      "  (l1): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a1): ELU(alpha=1.0)\n",
      "  (l2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (a2): ELU(alpha=1.0)\n",
      "  (output): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "tensor([[1.3462, 2.2317],\n",
      "        [1.2452, 2.0909],\n",
      "        [1.2287, 1.8858],\n",
      "        [1.4060, 2.1058],\n",
      "        [1.0786, 1.6400],\n",
      "        [1.7044, 2.9033],\n",
      "        [0.8829, 1.1785],\n",
      "        [1.4610, 2.3942],\n",
      "        [1.2641, 1.9120],\n",
      "        [1.0343, 1.4368]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-1.9164, grad_fn=<NegBackward0>)\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# start policy training\n",
    "scheduler = ExponentialLR(optimizer, gamma=gamma)\n",
    "q_x_a_train, q_x_a_test = dataset[\"q_x_a\"], dataset_test[\"q_x_a\"]\n",
    "# for _ in range(max_iter):\n",
    "_ = 0\n",
    "loss_epoch = 0.0\n",
    "nn_model.train()\n",
    "for x_, a_, r_, p, q_hat_, pi_0_ in training_data_loader:\n",
    "    optimizer.zero_grad()\n",
    "    pi = nn_model(x_)\n",
    "    print(nn_model)\n",
    "    print(pi)\n",
    "    loss = -_estimate_policy_gradient(\n",
    "        a=a_,\n",
    "        r=r_,\n",
    "        pscore=p,\n",
    "        q_hat=q_hat_,\n",
    "        pi_0=pi_0_,\n",
    "        pi=pi,\n",
    "    ).mean()\n",
    "    print(loss)\n",
    "    print('==============================')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_epoch += loss.item()\n",
    "train_loss.append(loss_epoch)\n",
    "scheduler.step()\n",
    "pi_train = predict(dataset)\n",
    "train_value.append((q_x_a_train * pi_train).sum(1).mean())\n",
    "pi_test = predict(dataset_test)\n",
    "test_value.append((q_x_a_test * pi_test).sum(1).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e1251-ebe4-4372-a504-57f14e9bfba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27f9f1-02bd-4d47-a59f-fad5959a0c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f275f8-dd15-475c-ac2d-aeae3aca965e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa2bfe-6772-47f9-b6eb-a0a4c9028521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36713faa-cc32-4c68-af24-8e81d6db6e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "095303ec-c299-4ca5-a796-ecee17c5ea08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0596, -0.0799],\n",
      "        [-0.0249, -0.0351],\n",
      "        [ 0.0053, -0.0109],\n",
      "        [ 0.2124,  0.0857],\n",
      "        [-0.3257, -0.1027],\n",
      "        [-0.0983, -0.0927],\n",
      "        [ 0.0868, -0.0966],\n",
      "        [-0.0470, -0.0554],\n",
      "        [ 0.0816, -0.1098],\n",
      "        [-0.0009, -0.0409]])\n",
      "====================\n",
      "tensor([[ 0.0074, -0.0447],\n",
      "        [ 0.1448,  0.0025],\n",
      "        [-0.0004, -0.0306],\n",
      "        [ 0.1130, -0.0888],\n",
      "        [-0.0588, -0.1069],\n",
      "        [ 0.0268, -0.0602],\n",
      "        [-0.0208, -0.0281],\n",
      "        [-0.0571, -0.0420],\n",
      "        [ 0.0383, -0.0757],\n",
      "        [ 0.0616, -0.0149]])\n",
      "====================\n",
      "tensor([[-0.0190, -0.0801],\n",
      "        [-0.1829, -0.1297],\n",
      "        [-0.1816, -0.1232],\n",
      "        [ 0.0910, -0.0658],\n",
      "        [ 0.0972, -0.0279],\n",
      "        [-0.0017, -0.0779],\n",
      "        [ 0.0076, -0.0085],\n",
      "        [-0.0736, -0.0966],\n",
      "        [-0.1470, -0.1289],\n",
      "        [ 0.0332, -0.0378]])\n",
      "====================\n",
      "tensor([[-0.1282, -0.1312],\n",
      "        [ 0.0358, -0.0466],\n",
      "        [ 0.1903,  0.0130],\n",
      "        [ 0.0206, -0.0025],\n",
      "        [-0.0259, -0.0749],\n",
      "        [ 0.1316, -0.0086],\n",
      "        [ 0.1337,  0.0175],\n",
      "        [ 0.0619, -0.0117],\n",
      "        [-0.0468, -0.0489],\n",
      "        [ 0.1731,  0.0714]])\n",
      "====================\n",
      "tensor([[ 0.0044, -0.0642],\n",
      "        [ 0.0915, -0.0073],\n",
      "        [ 0.0738, -0.0025],\n",
      "        [ 0.0837,  0.0348],\n",
      "        [-0.0814, -0.1183],\n",
      "        [-0.1101, -0.0988],\n",
      "        [ 0.2172,  0.0331],\n",
      "        [ 0.0446, -0.0424],\n",
      "        [-0.1468, -0.1028],\n",
      "        [ 0.0437, -0.0716]])\n",
      "====================\n",
      "tensor([[-0.0156, -0.0360],\n",
      "        [-0.0450, -0.1005],\n",
      "        [ 0.1089, -0.0606],\n",
      "        [-0.0798, -0.0790],\n",
      "        [ 0.1252, -0.0051],\n",
      "        [ 0.1152, -0.0155],\n",
      "        [-0.0933, -0.0883],\n",
      "        [-0.0457, -0.0188],\n",
      "        [ 0.0825, -0.0396],\n",
      "        [-0.2107, -0.1304]])\n",
      "====================\n",
      "tensor([[ 0.0522,  0.0129],\n",
      "        [ 0.0457, -0.1067],\n",
      "        [-0.0371, -0.0656],\n",
      "        [ 0.0932, -0.0121],\n",
      "        [-0.1980, -0.0850],\n",
      "        [ 0.0424, -0.0977],\n",
      "        [ 0.2449,  0.0703],\n",
      "        [ 0.0528, -0.0423],\n",
      "        [ 0.1633,  0.0565],\n",
      "        [ 0.0296, -0.0734]])\n",
      "====================\n",
      "tensor([[-0.0360, -0.0364],\n",
      "        [-0.0200, -0.0355],\n",
      "        [-0.1835, -0.1061],\n",
      "        [ 0.1899,  0.0893],\n",
      "        [-0.0390, -0.0493],\n",
      "        [-0.1201, -0.1113],\n",
      "        [-0.0526, -0.0578],\n",
      "        [-0.0247, -0.1002],\n",
      "        [-0.0481, -0.1408],\n",
      "        [-0.1141, -0.0571]])\n",
      "====================\n",
      "tensor([[-0.1015, -0.0858],\n",
      "        [-0.0870, -0.1005],\n",
      "        [-0.0091, -0.0911],\n",
      "        [-0.0532, -0.0724],\n",
      "        [ 0.0319, -0.0366],\n",
      "        [-0.1927, -0.1558],\n",
      "        [ 0.0366, -0.0509],\n",
      "        [ 0.0702, -0.0677],\n",
      "        [ 0.0243, -0.0807],\n",
      "        [-0.1913, -0.1353]])\n",
      "====================\n",
      "tensor([[-0.0640, -0.0793],\n",
      "        [-0.1496, -0.0854],\n",
      "        [ 0.0583, -0.0110],\n",
      "        [-0.0090, -0.0396],\n",
      "        [ 0.0803, -0.0338],\n",
      "        [-0.1366, -0.1058],\n",
      "        [ 0.0326, -0.0171],\n",
      "        [ 0.0718, -0.0390],\n",
      "        [ 0.0423, -0.0523],\n",
      "        [-0.0612, -0.1013]])\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for x_, a_, r_, p, q_hat_, pi_0_ in training_data_loader:\n",
    "    optimizer.zero_grad()\n",
    "    pi = nn_model(x_)\n",
    "\n",
    "    a=a_\n",
    "    r=r_\n",
    "    pscore=p\n",
    "    q_hat=q_hat_\n",
    "    pi_0=pi_0_\n",
    "    pi=pi\n",
    "    \n",
    "    current_pi = pi.detach()\n",
    "    log_prob = torch.log(pi + log_eps)\n",
    "    idx = torch.arange(a.shape[0], dtype=torch.long)\n",
    "    print(current_pi)\n",
    "    print('====================')\n",
    "\n",
    "    q_hat_factual = q_hat[idx, a]\n",
    "    iw = current_pi[idx, a] / pscore\n",
    "    estimated_policy_grad_arr = iw * (r - q_hat_factual) * log_prob[idx, a]\n",
    "    estimated_policy_grad_arr += torch.sum(q_hat * current_pi * log_prob, dim=1)\n",
    "\n",
    "    # imitation regularization\n",
    "    estimated_policy_grad_arr += imit_reg * log_prob[idx, a]\n",
    "\n",
    "    # return estimated_policy_grad_arr\n",
    "\n",
    "    # optimizer.zero_grad()\n",
    "    # pi = nn_model(x_)\n",
    "    # loss = -_estimate_policy_gradient(\n",
    "    #     a=a_,\n",
    "    #     r=r_,\n",
    "    #     pscore=p,\n",
    "    #     q_hat=q_hat_,\n",
    "    #     pi_0=pi_0_,\n",
    "    #     pi=pi,\n",
    "    # ).mean()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    # loss_epoch += loss.item()\n",
    "\n",
    "train_loss.append(loss_epoch)\n",
    "scheduler.step()\n",
    "pi_train = predict(dataset)\n",
    "train_value.append((q_x_a_train * pi_train).sum(1).mean())\n",
    "pi_test = predict(dataset_test)\n",
    "test_value.append((q_x_a_test * pi_test).sum(1).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076aa86-42c9-4d21-9122-22d93d6858f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
