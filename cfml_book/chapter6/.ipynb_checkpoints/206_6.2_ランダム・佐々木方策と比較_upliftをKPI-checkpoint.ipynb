{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 206_6.2_ランダム・佐々木方策と比較_upliftをKPI.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 プラットフォーム全体で観測される報酬を最適化する方策学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import check_random_state\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "plt.style.use('ggplot')\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from dataset import generate_synthetic_data2\n",
    "# from policylearners import IPSBasedGradientPolicyLearner, CateBasedGradientPolicyLearner\n",
    "from policylearners_sasaken_edit import IPSBasedGradientPolicyLearner, CateBasedGradientPolicyLearner_by_sk\n",
    "from utils import softmax\n",
    "\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ランダム方策と佐々木方策の関数定義を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ランダム方策"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_random_pg(num_actions, test_data):\n",
    "    q_x_a_1, q_x_a_0 = test_data[\"q_x_a_1\"], test_data[\"q_x_a_0\"]\n",
    "    num_data = test_data['num_data']\n",
    "    pi_random = np.full((num_data, num_actions), 1/num_actions)\n",
    "    radom_pg = (pi_random * q_x_a_1 + (1 - pi_random) * q_x_a_0).sum(1).mean()\n",
    "    return radom_pg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 佐々木方策"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "# 各行に対してsoftmaxを適用\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # オーバーフロー防止\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "def calc_sasaken_pg(offline_logged_data, test_data):\n",
    "    # データ整形: 観測される情報のみを持つDataFrameを生成する\n",
    "    x = offline_logged_data['x']\n",
    "    a_mat = offline_logged_data['a_mat']\n",
    "    _q_x_a_0 = offline_logged_data['q_x_a_0'] # ノイズはないがログデータとして各コンテンツの試聴時間は得られているものとする\n",
    "    _q_x_a_1 = offline_logged_data['q_x_a_1'] # ノイズはないがログデータとして各コンテンツの試聴時間は得られているものとする\n",
    "    num_actions = _q_x_a_0.shape[1]\n",
    "    # 結合\n",
    "    combined = np.hstack([x, a_mat, _q_x_a_0, _q_x_a_1])\n",
    "    # カラム名の生成\n",
    "    columns = [f'x_{i}' for i in range(x.shape[1])] + [f'a_{i}' for i in range(a_mat.shape[1])]\n",
    "    columns += [f'q_x_a_contents_{i}_not_recommended' for i in range(num_actions)]\n",
    "    columns += [f'q_x_a_contents_{i}_recommended' for i in range(num_actions)]\n",
    "    # DataFrame 作成\n",
    "    df = pd.DataFrame(combined, columns=columns)\n",
    "\n",
    "    # 初期化（NaNで埋める）\n",
    "    for i in range(num_actions):\n",
    "        df[f'observed_r_contents_{i}'] = np.nan\n",
    "    \n",
    "    # 各行でどのアクションが推薦されたか（a_i == 1の列）を使って代入\n",
    "    for i in range(num_actions):\n",
    "        recommended_mask = df[f'a_{i}'] == 1\n",
    "        for j in range(num_actions):\n",
    "            colname = f'observed_r_contents_{j}'\n",
    "            source_col = (\n",
    "                f'q_x_a_contents_{j}_recommended' if i == j\n",
    "                else f'q_x_a_contents_{j}_not_recommended'\n",
    "            )\n",
    "            df.loc[recommended_mask, colname] = df.loc[recommended_mask, source_col]\n",
    "\n",
    "    model_list = []\n",
    "    for a_val in range(num_actions):\n",
    "    \n",
    "        # 説明変数: x0〜x4 + a（処置変数）\n",
    "        feature_cols = ['x_0', 'x_1', 'x_2', 'x_3', 'x_4', f'a_{a_val}']\n",
    "        X = df[feature_cols]\n",
    "        y = df[f'observed_r_contents_{a_val}']  # 目的変数\n",
    "        \n",
    "        # 学習\n",
    "        tmp_model = RandomForestRegressor(random_state=42)\n",
    "        tmp_model.fit(X, y)\n",
    "    \n",
    "        model_list.append(tmp_model)\n",
    "\n",
    "    # testデータをデータフレームに変換\n",
    "    test_data_df = pd.DataFrame(test_data['x'], columns=[\n",
    "        'x_0',\n",
    "        'x_1',\n",
    "        'x_2',\n",
    "        'x_3',\n",
    "        'x_4',\n",
    "    ])\n",
    "    \n",
    "    for a_val in range(num_actions):\n",
    "        # a_0\n",
    "        ## a_0 not recommended\n",
    "        tmp_test_data_df = test_data_df.copy()\n",
    "        tmp_test_data_df[f'a_{a_val}'] = 0\n",
    "        test_data_df[f'q_x_a_hat_contents_{a_val}_when_a_{a_val}_not_recommended'] = model_list[a_val].predict(\n",
    "            tmp_test_data_df[['x_0', 'x_1', 'x_2', 'x_3', 'x_4', f'a_{a_val}']]\n",
    "        )\n",
    "        tmp_test_data_df = test_data_df.copy()\n",
    "        \n",
    "        ## a_0 recommended\n",
    "        tmp_test_data_df[f'a_{a_val}'] = 1\n",
    "        test_data_df[f'q_x_a_hat_contents_{a_val}_when_a_{a_val}_recommended'] = model_list[a_val].predict(\n",
    "            tmp_test_data_df[['x_0', 'x_1', 'x_2', 'x_3', 'x_4', f'a_{a_val}']]\n",
    "        )\n",
    "    for a_val in range(num_actions):\n",
    "        test_data_df[f'uplift_contents_{a_val}_by_recommend'] = test_data_df[f'q_x_a_hat_contents_{a_val}_when_a_{a_val}_recommended'] - test_data_df[f'q_x_a_hat_contents_{a_val}_when_a_{a_val}_not_recommended']\n",
    "\n",
    "    # ソフトマックス適用\n",
    "    q_hat = test_data_df[[f\"uplift_contents_{a_val}_by_recommend\" for a_val in range(num_actions)]].values\n",
    "    pi_sasaken = softmax(q_hat)\n",
    "    q_x_a_1, q_x_a_0 = test_data[\"q_x_a_1\"], test_data[\"q_x_a_0\"]\n",
    "    sasaken_pg = (pi_sasaken * q_x_a_1 + (1 - pi_sasaken) * q_x_a_0).sum(1).mean()\n",
    "\n",
    "    q_x_a_1_hat = np.array(test_data_df[[f\"q_x_a_hat_contents_{a_val}_when_a_{a_val}_recommended\" for a_val in range(num_actions)]])\n",
    "    q_x_a_0_hat = np.array(test_data_df[[f\"q_x_a_hat_contents_{a_val}_when_a_{a_val}_not_recommended\" for a_val in range(num_actions)]])\n",
    "\n",
    "    return sasaken_pg, q_x_a_1_hat, q_x_a_0_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ログデータ(トレーニングデータ)のサイズ$n$を変化させたときの方策性能の変化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## シミュレーション設定\n",
    "num_runs = 50 # シミュレーションの繰り返し回数\n",
    "\n",
    "dim_x = 5 # 特徴量xの次元\n",
    "num_actions = 10 # 行動数, |A|\n",
    "beta = -0.1 # データ収集方策のパラメータ\n",
    "max_iter = 21 # エポック数\n",
    "test_data_size = 50000 # テストデータのサイズ\n",
    "random_state = 12345\n",
    "torch.manual_seed(random_state)\n",
    "random_ = check_random_state(random_state)\n",
    "\n",
    "num_actions_list = [2, 10] # 行動数\n",
    "\n",
    "num_data_list = [100, 500, 1000, 2000] # トレーニングデータのサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_data_list = []\n",
    "for num_actions in num_actions_list:\n",
    "    ## 期待報酬関数を定義するためのパラメータを抽出\n",
    "    theta_1 = random_.normal(size=(dim_x, num_actions))\n",
    "    M_1 = random_.normal(size=(dim_x, num_actions))\n",
    "    b_1 = random_.normal(size=(1, num_actions))\n",
    "    theta_0 = random_.normal(size=(dim_x, num_actions))\n",
    "    M_0 = random_.normal(size=(dim_x, num_actions))\n",
    "    b_0 = random_.normal(size=(1, num_actions))\n",
    "    ## 学習された方策の真の性能を近似するためのテストデータを生成\n",
    "    test_data = generate_synthetic_data2(\n",
    "        num_data=test_data_size, beta=beta,\n",
    "        theta_1=theta_1, M_1=M_1, b_1=b_1, theta_0=theta_0, M_0=M_0, b_0=b_0,\n",
    "        dim_context=dim_x, num_actions=num_actions, random_state = random_state\n",
    "    )\n",
    "    pi_0, q_x_a_1, q_x_a_0 = test_data[\"pi_0\"], test_data[\"q_x_a_1\"], test_data[\"q_x_a_0\"]\n",
    "    pi_0_value = (pi_0 * q_x_a_1 + (1. - pi_0) * q_x_a_0).sum(1).mean()\n",
    "\n",
    "    result_df_list = []\n",
    "    for num_data in num_data_list:\n",
    "        test_policy_value_list = []\n",
    "        for _ in tqdm(range(num_runs), desc=f\"num_data={num_data}...\"):\n",
    "            ## データ収集方策が形成する分布に従いログデータを生成\n",
    "            offline_logged_data = generate_synthetic_data2(\n",
    "                num_data=num_data, beta=beta,\n",
    "                theta_1=theta_1, M_1=M_1, b_1=b_1, theta_0=theta_0, M_0=M_0, b_0=b_0,\n",
    "                dim_context=dim_x, num_actions=num_actions, random_state = _\n",
    "            )\n",
    "\n",
    "            true_value_of_learned_policies = dict()\n",
    "            true_value_of_learned_policies[\"logging\"] = pi_0_value\n",
    "\n",
    "            ## ログデータを用いてオフ方策学習を実行する\n",
    "            ### 勾配ベースのアプローチ (IPS推定量で方策勾配を推定)\n",
    "            ips = IPSBasedGradientPolicyLearner(\n",
    "                dim_x=dim_x, num_actions=num_actions, max_iter=max_iter, random_state=random_state + _\n",
    "            )\n",
    "            ips.fit(offline_logged_data, test_data)\n",
    "            pi_ips = ips.predict(test_data)\n",
    "            true_value_of_learned_policies[\"ips-pg\"] = (pi_ips * q_x_a_1 + (1 - pi_ips) * q_x_a_0).sum(1).mean()\n",
    "\n",
    "            sasaken_pg, q_x_a_1_hat, q_x_a_0_hat = calc_sasaken_pg(offline_logged_data=offline_logged_data, test_data=test_data)\n",
    "            ### 勾配ベースのアプローチ (新たに開発した推定量で方策勾配を推定)\n",
    "            cate = CateBasedGradientPolicyLearner_by_sk(\n",
    "                dim_x=dim_x, num_actions=num_actions, max_iter=max_iter, random_state=random_state + _\n",
    "            )\n",
    "            cate.fit(offline_logged_data, test_data,\n",
    "                q_x_a_1_hat,\n",
    "                q_x_a_0_hat\n",
    "            )\n",
    "            pi_cate = cate.predict(test_data)\n",
    "            true_value_of_learned_policies[\"cate-pg\"] = (pi_cate * q_x_a_1 + (1 - pi_cate) * q_x_a_0).sum(1).mean()\n",
    "            \n",
    "            ### ランダム\n",
    "            true_value_of_learned_policies[\"random-pg\"] = calc_random_pg(num_actions=num_actions, test_data=test_data)\n",
    "\n",
    "            ### 佐々木オリジナル\n",
    "            true_value_of_learned_policies[\"sasaken-pg\"] = sasaken_pg\n",
    "\n",
    "            \n",
    "            test_policy_value_list.append(true_value_of_learned_policies)\n",
    "            \n",
    "        ## シミュレーション結果の集計\n",
    "        result_df = DataFrame(test_policy_value_list).stack().reset_index(1)\\\n",
    "            .rename(columns={\"level_1\": \"method\", 0: \"value\"})\n",
    "        result_df[\"num_data\"] = num_data\n",
    "        result_df[\"pi_0_value\"] = pi_0_value\n",
    "        result_df[\"rel_value\"] = result_df[\"value\"] / pi_0_value\n",
    "        result_df_list.append(result_df)\n",
    "    result_df_data_list.append(pd.concat(result_df_list).reset_index(level=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_list = plt.subplots(1, len(num_actions_list), figsize=(28, 9.5), tight_layout=True)\n",
    "for i, (num_actions, result_df) in enumerate(zip(num_actions_list, result_df_data_list)):\n",
    "    ax = ax_list[i]\n",
    "    sns.lineplot(\n",
    "        linewidth=9,\n",
    "        markersize=30,\n",
    "        markers=True,\n",
    "        x=\"num_data\",\n",
    "        y=\"value\",\n",
    "        hue=\"method\",\n",
    "        style=\"method\",\n",
    "        ax=ax,\n",
    "        palette=[\"tab:grey\", \"tab:red\", \"tab:purple\", \"tab:blue\", \"tab:pink\"],\n",
    "        legend=False,\n",
    "        data=result_df,\n",
    "    )\n",
    "    ax.set_title(f\"行動の数: {num_actions}\", fontsize=50)\n",
    "    # yaxis\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"学習された方策の性能\", fontsize=45)\n",
    "    else:\n",
    "        ax.set_ylabel(\"\")\n",
    "    ax.tick_params(axis=\"y\", labelsize=30)\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "    # xaxis\n",
    "    ax.set_xlabel(\"トレーニングデータ数$n$\", fontsize=45)\n",
    "    ax.set_xticks(num_data_list)\n",
    "    ax.set_xticklabels(num_data_list, fontsize=30)\n",
    "    ax.xaxis.set_label_coords(0.5, -0.11)\n",
    "# fig.legend([\"データ収集方策\", \"IPS\", \"New\"], fontsize=50, bbox_to_anchor=(0.5, 1.13), ncol=5, loc=\"center\")\n",
    "\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color=\"tab:grey\", marker='o', markersize=30, linestyle='None', label=\"データ収集方策\"),\n",
    "    Line2D([0], [0], color=\"tab:red\", marker='X', markersize=30, linestyle='None', label=\"IPS\"),\n",
    "    Line2D([0], [0], color=\"tab:purple\", marker='s', markersize=30, linestyle='None', label=\"New\"),\n",
    "    Line2D([0], [0], color=\"tab:blue\", marker='+', markersize=30, markeredgewidth=10, linestyle='None', label=\"Random\"),\n",
    "    Line2D([0], [0], color=\"tab:pink\", marker='D', markersize=30, linestyle='None', label=\"Sasaken\"),\n",
    "]\n",
    "\n",
    "fig.legend(\n",
    "    handles=custom_lines,\n",
    "    fontsize=50,\n",
    "    bbox_to_anchor=(0.5, 1.13),\n",
    "    ncol=5,\n",
    "    loc=\"center\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
